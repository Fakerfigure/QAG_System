# Perception-R1: Pioneering Perception Policy with Reinforcement Learning  

En ${\bf Y}{\bf u}^{1,\P}$ , Kangheng $\ensuremath{\mathbf{Lin^{2,\P}}}$ , Liang Zhao3,¶, Jisheng $\mathbf{Yin^{3}}$ , Yana Wei4, Yuang $\mathbf{Peng^{5}}$ , Haoran Wei3, Jianjian $\mathbf{Sun^{3}}$ , Chunrui $\mathbf{Han^{3}}$ , Zheng $\mathbf{Ge^{3}}$ , Xiangyu Zhang3, Daxin Jiang3, Jingyu Wang2, Wenbing Tao1† 1Huazhong University of Science and Technology 2Beijing University of Posts and Telecommunications 3StepFun 4Johns Hopkins University 5Tingshua University {yuen, wenbingtao}@hust.edu.cn https://github.com/linkangheng/PR1  

# Abstract  

Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in MLLM post-training for perception policy learning. While promising, our initial experiments reveal that incorporating a thinking process through RL does not consistently lead to performance gains across all visual perception tasks. This leads us to delve into the essential role of RL in the context of visual perception. In this work, we return to the fundamentals and explore the effects of RL on different perception tasks. We observe that the perceptual perplexity is a major factor in determining the effectiveness of RL. We also observe that reward design plays a crucial role in further approaching the upper limit of model perception. To leverage these findings, we propose Perception-R1, a scalable RL framework using GRPO during MLLM post-training. With a standard Qwen2-VL-2B-Instruct, Perception-R1 achieves $+4.\ensuremath{\mathrm{2\%}}$ on ${\mathrm{RefCOCO+}}$ , $\mathbf{+17.9\%}$ on PixMo-Count, $+4.2\%$ on PageOCR, and notably, $\mathbf{31.9\%}$ AP on COCO2017 val for the first time, establishing a strong baseline for perception policy learning.  

# 1 Introduction  

“We do not see the world as it is, but as we are — or as we are conditioned to see it.”  

Stephen R. Covey  

The landscape of large language model (LLM) has undergone a paradigm shift from non-reasoning foundation model, e.g., GPT-4/4o [44, 19], DeepSeek-V3 [33], to strongly reasoning model, e.g., OpenAI o1/o3 [45], DeepSeek-R1 [12], and Kimi-1.5 [57]. DeepSeek-R1, in particular, introduced a simple yet effective rule-based reinforcement learning (RL) approach [55], enabling emergent reasoning patterns without relying on traditional scaffolding techniques such as Monte Carlo Tree Search (MCTS) [17, 67] or Process Reward Models (PRM) [31]. This has catalyzed a new revolution in LLM post-training techniques, prompting researchers to develop more powerful reasoning language models [42, 24].  

Despite these advancements, current explorations predominantly focus on the purely linguistic domain, and the unimodal nature of these reasoning models limits their ability to engage with the world in a truly perceptive way. To bridge this gap, this work takes a pioneering step in exploring the potential of perception policy learning within multimodal LLMs [61, 3] from lens of RL. While transferring RL techniques with reasoning processes, i.e., chain-of-thought [66], from the language domain shows promise on certain visual tasks, our empirical studies reveal that this approach is not universally effective. This inevitably prompts us to reexamine the role that RL play in visual perception tasks, and how the utilization of RL can lead to better and scalable perception policy.  

The current understanding of RL as a post-training technique is primarily grounded in purely linguistic tasks [24] and language-centric multimodal tasks [10]. However, the characteristics of visual perception tasks are fundamentally distinct from those of natural language, necessitating a revised understanding of RL in the context of visual perception. Specifically, visual perception possesses two unique properties, as follows:  

• Visual perception is embodied in the objective physical world. It possesses definite physical truth values, e.g., points, lines, or bounding boxes, but it lacks semantics compared to language. • Visual perception, e.g., visual grounding and counting, are mostly "single-step" direct predictions. It lacks structured reasoning search space for RL exploration.  

These two characteristics determine that the application of RL to visual perception will have different properties from pure language [24] and language-centric multimodal [39, 41] approaches. In this work, we delve into the RL post-training of MLLM in the domain of visual perception, and further complements and extends the above understanding. Through extensive experimental analysis, we have uncovered several bitter yet valuable findings.  

• Explicit thinking process (CoT) during RL is not necessary for current perception policy. (§ 5.2) We observe that the model without thinking process performs better than the one with thinking process. • Reward design plays a pivotal role in perception policy learning. (§ 5.3) An appropriate reward function will lead to a healthier learning curve and explore stronger perceptual patterns of MLLM. • Perceptual perplexity determines RL superiority over SFT. $(\S\ S.2)$ We observe that RL can bring more significant improvement compared to SFT on more complex visual tasks, e.g., object detection.  

Driven by these findings, we present a simple, effective, and scalable RL framework, i.e., Perception$_{R I}$ , for efficient perception policy learning. Inspired by mainstream language reasoning models [12, 57], Perception-R1 applies rule-based RL algorithm GRPO [55] during MLLM post-training stage. With a vanilla Qwen2-VL-2B-Instruct [61], Perception-R1 achieves significant improvement on multiple visual perception benchmarks, e.g., $+4.{\dot{\bf2}}\%$ on ${\mathrm{RefCOCO+}}$ [40], $\mathbf{+17.9\%}$ on PixMoCount [13], and $+4.\Bar{2}\%$ F1-score on PageOCR [34]. More importantly, Perception-R1 serves as the first time to enable a pure MLLM to reach $\mathbf{31.9\%}$ mAP on the object detection benchmark COCO2017 [32] val, showcasing the great potential of general foundation models to surpass expert models in mainstream visual tasks. We hope our method, results, and analysis will inspire future research on perception policy learning with RL.  

# 2 Related Works  

Multimodal Foundation and Reasoning Models. Recently, vision-language models [37, 3, 73, 70] have demonstrated remarkable capabilities in visual comprehension [64, 68] and generation [14, 48] through large-scale pretraining [2, 61] and visual instruction tuning [37, 35]. These models integrate visual modalities into a unified semantic space via visual encoders [49] and adapters [11, 37], while leveraging auto-regressive large language models [59, 1] as decoders for output generation. Despite the advancements in multimodal foundation models, their visual reasoning capabilities remain in an early developmental stage. Recent approaches [8, 39, 41] have explored reinforcement learning (RL) post-training to enhance visual reasoning. However, they primarily focus on language-centric tasks such as ambiguous reference resolution [39] and geometric problem-solving [41], while overlooking critical aspects of perception-driven reasoning. In this work, we take a pioneering step in utilizing RL for perception policy learning, aiming to bridge this gap and advance multimodal reasoning.  

Visual Perception in Multimodal Models. Visual Perception, as a concept in the field of computer vision [21, 52, 20, 69, 29], refers to the process of interpreting and understanding sensory, i.e., vision, information from the real-word. In the context of multimodal LLMs (MLLM), visual perception plays a crucial role in enabling the models to integrate, comprehend and reason visual information from the image or video. Existing MLLM generally enhance their visual perception capabilities by designing more advanced visual perception architectures [63, 64], more suitable visual-language modeling strategies [70, 68], and more sophisticated post-training techniques [74]. This work aims to explore the potential of further enhancing visual perception from the perspective of RL.  

RL-based Post-training in LLMs and MLLMs. Reinforcement learning (RL) has emerged as a pivotal paradigm for refining LLMs through alignment with human preferences and task-specific objectives. Prominent approaches like Reinforcement Learning from Human Feedback (RLHF) [46] and Direct Preference Optimization (DPO) [50] have demonstrated remarkable success in enhancing safety, coherence, and instruction-following capabilities of LLMs [43, 47, 44] and MLLMs [74, 60]. Recently, rule-based RL techniques, represented by GRPO [55], have demonstrated the potential for large-scale RL applications. LLMs have officially entered the era of strongly reasoning models. Subsequently, MLLMs [8, 39, 41] have also quickly followed this technology. However, so far, there has been no exciting, true "Aha Moment" in the multimodal domain. This study aims to investigate the potential contributions of RL to multimodal models, focusing on visual perception.  

# 3 Preliminaries  

Perception Policy Definition. The goal of perception policy in visual-language context is enabling the model to first $(i)$ extract and understand visual information from the environment [37, 68], then $(\ddot{u})$ perform logical reasoning based on this understanding [73, 70] to (iii) accomplish specific tasks and further interact with the environment [5, 22]. In this work, we aim to empower the model to deal with a series of pure visual, e.g., counting, detection, and visual-language, e.g., grounding, optical character recognition (OCR), tasks through perception policy learning.  

Group Relative Policy Optimization (GRPO [55]) is a rule-based reinforcement learning algorithm tailored for post-training LLMs. Its core idea is to use group relative rewards to optimize the policy, eliminating the need for a separate critic model [54]. Specifically, GRPO samples multiple outputs $(\mathbf{o1}\sim\mathbf{o_{g}}$ in Figure 1) from the old policy for the same input, calculates the average reward of these outputs as the baseline, and uses the relative rewards to guide policy updates. The optimization objective of GRPO can be formulated as following:  

$$
\begin{array}{l}{\displaystyle\mathcal{I}_{\mathrm{GRPO}}(\theta)=\mathbb{E}_{[q\sim P(Q),\{\sigma_{i}\}_{i=1}^{G}\sim\pi_{\theta_{\mathrm{sh}}(\theta)}(\boldsymbol{O}\mid q)]}}\\ {\displaystyle\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|\sigma_{i}|}\sum_{t=1}^{|\sigma_{i}|}\left\{\operatorname*{min}\left[\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_{\mathrm{sh}}}^{i,t}}\hat{A}_{i,t},\mathrm{clip}\left(\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_{\mathrm{sh}}}^{i,t}},1-\epsilon,1+\epsilon\right)\hat{A}_{i,t}\right]-\beta\mathbb{D}_{\mathrm{KL}}[\pi_{\theta}||\pi_{\mathrm{ref}}]\right\},}\end{array}
$$  

$$
\mathbb{D}_{\mathrm{KL}}\left[\pi_{\theta}\lVert\pi_{\mathrm{ref}}\right]=\frac{\pi_{\mathrm{ref}}\left(o_{i,t}\middle|q,o_{i,<t}\right)}{\pi_{\theta}\left(o_{i,t}\middle|q,o_{i,<t}\right)}-\log\frac{\pi_{\mathrm{ref}}\left(o_{i,t}\middle|q,o_{i,<t}\right)}{\pi_{\theta}\left(o_{i,t}\middle|q,o_{i,<t}\right)}-1,
$$  

where $\epsilon$ and $\beta$ are hyper-parameters, and $\hat{A}_{i,t}$ is the advantage, computed using a group of rewards $\{r_{1},r_{2},\cdot\cdot\cdot,r_{G}\}$ corresponding to the outputs within each group. Refer to [12, 55] for more details.  

# 4 Perception-R1  

In a nutshell, our Perception-R1 applies the rule-based RL algorithm GRPO [55] to the post-training stage of MLLM and optimizes the reward modeling to support perception policy learning. Figure 1 illustrates the idea, more apprach and implementation details introduced next.  

# 4.1 Rule-based Reward Modeling  

The reward function serves as the principal training signal in reinforcement learning (RL), directing the optimization process. Existing LLM methods [12, 57, 24] basically apply a highly resilient, rule-based reward system consisting of only two reward types: Format Reward and Answer Reward.  

Format Reward. In existing LLM and MLLM, the output format is comprised of two essential components: the final output format and the intermediate reasoning process format. The reward for  

Figure 1: Illustration of Perception-R1 framework. Following DeepSeek-R1 [12], we prompt MLLM model to generate several rollout responses and apply GRPO [55] during post-training stage.  

the final output is defined in accordance with specific task requirements and is typically encapsulated within <answer></answer> tags, whereas the reward for the intermediate reasoning process generally mandates that the reasoning steps be enclosed within <think></think> tags. Formally,  

$$
S_{\mathrm{format}}={\binom{1,\qquad{\mathrm{if~format~is~correct}}}{-1,\quad{\mathrm{if~format~is~incorrect}}}}
$$  

In Perception-R1, we follow this setting. A subtle difference emerges that visual perception task frequently require the output of object coordinates, e.g., bounding box, lines, or points. Consequently, the output format must be strictly constrained to the [x1,y1,x2,y2] structure.  

Answer Reward. The Answer Reward pertains to the correctness of model-generated responses, serving as a central consideration in reward design. Typically, outputs from language models are abstract and semantically rich, requiring validation through external mechanisms such as code-based ADE [12] or mathematical answer verification [55]. In contrast, visual perception tasks benefit from clearly defined physical ground truths, which simplify the development of a robust reward function.  

Perception-R1 diverges from LLM approaches by anchoring the reward mechanism in visual discrimination. This departure is pivotal, as it replaces the often implicit and subjective feedback mechanisms typical of language models with an explicit, quantifiable metric. Formally, discriminative reward $r_{i}$ can be represented as:  

$$
r_{i}=\Phi(o_{i},z),
$$  

where $\Phi(\cdot)$ indicates the discriminative function, for example, IoU for bounding box and euclidean distance for point. By leveraging visual discrimination, we provide the model with a clear and objective feedback signal, ensuring the model’s policy update with precise measured margin.  

# 4.2 Multi-Subject Reward Matching  

In natural environments, physical objects rarely appear in isolation and instead frequently co-occur in groups. This inherent complexity gives rise to a challenge we define as reward matching, which entails aligning the model’s output with the corresponding ground truth before reward computation. Specifically, when prompting the model to predict the attributes of multiple subjects within an image, e.g., points and bounding box, it is necessary to determine the appropriate ground truth reference for each subject to ensure accurate reward assignment.  

Formally, let $\boldsymbol{y}=\{y_{i}\}_{i=1}^{N}$ denote the set of predicted attributes for $N$ subjects, and let $z=\{z_{j}\}_{j=1}^{M}$ represent the corresponding ground truth attributes.We model the reward matching problem as a bipartite graph matching task, where one set of nodes corresponds to predictions and the other to ground truths. The edge weight between a prediction $y_{i}$ and a ground truth $t_{j}$ is determined by the reward function $\Phi(y_{i},z_{j})$ defined in Eq. 3, which measures their similarity or compatibility. The objective is to find the optimal assignment that maximizes the total reward:  

$$
\hat{\sigma}=\underset{\sigma\in\Omega_{N}}{\arg\operatorname*{max}}\sum_{i=1}^{N}\Phi\left(y_{i},z_{\sigma\left(i\right)}\right),
$$  

where $\Omega_{N}$ is the set of all valid assignments between predictions and ground truths. To solve this optimization problem efficiently, we employ the Hungarian algorithm [27], a well-established method for bipartite graph matching that guarantees the optimal pairing by maximizing the overall reward (or equivalently, minimizing the cost). This ensures that each predicted attribute is accurately matched with its corresponding ground truth, thereby optimizing the reward computation process.  

After the optimal reward assignment is determined, we calculate the answer reward by aggregating the individual rewards for each subject. Mathematically, the overall reward score is defined as:  

$$
\begin{array}{r l r}{\lefteqn{S_{\mathrm{answer}}=\frac{1}{N}\sum_{i=1}^{N}\Phi\left(y_{i},z_{\hat{\sigma}(i)}\right),}}\\ &{}&{\quad S_{\mathrm{total}}=S_{\mathrm{format}}+S_{\mathrm{answer}}}\end{array}
$$  

where $\hat{\sigma}$ is the optimal assignment obtained via the Hungarian algorithm.In Perception-R1, we primarily use reward matching for visual counting and object detection tasks, as these involve multiple objects.  

# 4.3 Perception-R1 Configuration  

Model Setting. Our model implementation follows Qwen2-VL [61]. We mainly use the Qwen2-VLInstruct-2B as the baseline model.We also utilize Qwen2.5-VL-3B-Instruct [3] for training object detection tasks, due to its specialized optimization for localizing bounding boxes. The input image resolution for Qwen2-VL is dynamic cooperated with 2D-RoPE [56].  

Task and Data Setting. Given that Perception-R1 is primarily oriented towards pure visual and visuallanguage tasks, we select several mainstream and representative downstream tasks for perception policy learning, specifically including visual grounding, e.g., refCOCO [7 $1]/+[71]/\mathrm{g}$ [40], OCR, i.e., PageOCR [34], visual counting, i.e., Pixmo-Count [13], and object detection, i.e., COCO2017 [32]. For each task, a subset $5k\sim10k)$ of samples are respectively extracted as base data for individual post-training. More details are in appendix A.1.  

Training Setting. We focus on the RL-based post-training stage of MLLM. All the selected base models have already undergone pre-training and SFT stage. During RL stage, the initial learning rate is set as $1e-6$ with 8 rollouts by default and a batch size of 1. The following are some important hyper-parameters during post-training. Prompts detailed settings are in the appendix A.1.  

<html><body><table><tr><td>Gradient Accmulation</td><td>Rollout G</td><td>KL Coefficient</td><td>Max Response Len</td><td>Temperature</td></tr><tr><td>2</td><td>8</td><td>0.04</td><td>2048</td><td>1.0</td></tr></table></body></html>  

Reward Setting. We tailor distinct discriminative rewards for various visual perception tasks. For the grounding task, the reward is based on the Intersection over Union (IoU) between the predicted output and the ground truth. In the counting task, we adopt a paradigm similar to Qwen2.5-VL, which first detects points and then counts them. Here, the reward is derived from the Euclidean distance computed during reward matching. For OCR, the edit distance serves as the primary reward metric. Lastly, in object detection, we combine multiple rewards: an object number reward based on the F1 score, a location reward using IoU, and a binary classification reward with a missing penalty.  

Sampling Setting. Following Kimi-1.5 [57], we adopt a curriculum sampling strategy that begins with easier data and gradually transitions to more challenging examples. Specifically, for the object detection task, we first conduct offline training on the COCO dataset to compute reward values. Based on the selected rewards, i.e., number reward, we partition the dataset accordingly. As training advances, we progressively replace the data with more difficult samples (i.e., those associated with lower rewards) while concurrently increasing the rollout to broaden the model’s exploration space.  

Table 1: Visual grounding benchmark evaluation. To comprehensively assess the model’s grounding capability, we select referring experssion comprehension (REC) benchmark, i.e., RefCOCO [71], $_\mathrm{RefCOCO+[71]}$ , and RefCOCOg[40] for evaluation. The expert model is denoted in gray.   


<html><body><table><tr><td></td><td></td><td colspan="12">RefCOCO</td></tr><tr><td>method</td><td></td><td colspan="3">Sizeval@5otestA@5otestB@50val@75testA@7stestB@75Val@g5 testA@g5testB@g5valArg testApgtestBAvg</td><td colspan="3"></td><td colspan="3"></td><td colspan="3"></td></tr><tr><td>MDETR [25]</td><td></td><td>87.5</td><td>90.4</td><td>82.6</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>OFA [62]</td><td></td><td>88.4</td><td>90.6</td><td>83.3</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LLaVA-1.5 [35]</td><td>7B</td><td>49.1</td><td>54.9</td><td>43.3</td><td>10.7</td><td>13.6</td><td>6.9 0.4</td><td>0.3</td><td>0.3</td><td>20.1</td><td>22.9</td><td>16.8</td></tr><tr><td>LLaVA-NeXT [36]7B</td><td></td><td>82.5</td><td>88.4</td><td>74.0</td><td>45.7</td><td>54.8</td><td>35.6 1.9</td><td>2.6</td><td>0.7</td><td>43.4</td><td>48.6</td><td>36.8</td></tr><tr><td>LLaVA-OV [28]</td><td>7B</td><td>73.0</td><td>82.3</td><td>63.5</td><td>24.2</td><td>29.6</td><td>15.9 0.5</td><td>0.5</td><td>0.5</td><td>32.6</td><td>37.5</td><td>26.6</td></tr><tr><td>Qwen2-VL [61]</td><td>2B</td><td>86.8</td><td>89.6</td><td>82.0</td><td>77.2</td><td>80.6</td><td>70.1 33.0</td><td>35.7</td><td>26.9</td><td>65.7</td><td>68.6</td><td>59.7</td></tr><tr><td>Perception-R1</td><td>2B</td><td>89.1</td><td>91.4</td><td>84.5</td><td>79.5</td><td>83.6</td><td>72.4 35.0</td><td>38.5</td><td></td><td>28.8</td><td>67.9 71.2</td><td>61.9</td></tr><tr><td>method</td><td colspan="16">RefCOCO+ sizeval@5o testA@5otestB@50val@7stestA@7stestB@75val@gstestA@gstestB@g5valArg testApgtestBAg</td></tr><tr><td>MDETR [25]</td><td></td><td>81.1</td><td>85.5</td><td>72.9</td><td colspan="2"></td><td colspan="3"></td><td colspan="3"></td></tr><tr><td>OFA [62]</td><td></td><td>81.3</td><td>87.1</td><td>74.2</td><td colspan="2"></td><td></td><td colspan="2"></td><td></td><td colspan="2"></td></tr><tr><td>LLaVA-1.5 [35]</td><td>7B</td><td>42.4</td><td>49.7</td><td>36.4</td><td>9.8</td><td>12.4</td><td>6.4</td><td>0.5 0.5</td><td>0.2</td><td></td><td>17.6 20.8</td><td>14.3</td></tr><tr><td>LLaVA-NeXT [36]7B</td><td></td><td>74.5</td><td>84.0</td><td>64.7</td><td>41.5</td><td>51.8</td><td>30.0 1.9</td><td>2.7</td><td>1.0</td><td>39.3</td><td>46.2</td><td>31.9</td></tr><tr><td>LLaVA-OV [28]</td><td>7B</td><td>65.8</td><td>79.0</td><td>57.2</td><td>23.6</td><td>28.8</td><td>15.3</td><td>0.6 0.6</td><td></td><td>0.4</td><td>30.0 36.1</td><td>24.3</td></tr><tr><td>Qwen2-VL [61]</td><td>2B</td><td>77.1</td><td>82.5</td><td>70.1</td><td>68.7</td><td>73.8</td><td>60.0</td><td>29.4 32.3</td><td></td><td>23.0 58.4</td><td>62.9</td><td>51.0</td></tr><tr><td>Perception-R1</td><td>2B</td><td>81.7</td><td>86.8</td><td>74.3</td><td>73.6</td><td>79.3</td><td>64.2</td><td>32.6 36.9</td><td>26.7</td><td>62.6</td><td>67.7</td><td>55.1</td></tr><tr><td></td><td></td><td colspan="8"></td><td></td><td></td><td></td></tr><tr><td>method</td><td></td><td>sizeval@50 test@50</td><td colspan="8">val@75 test@75</td><td>valAvg testAvg</td></tr><tr><td>MDETR [25]</td><td></td><td>83.3 83.3</td><td colspan="2"></td><td colspan="2">1</td><td colspan="3">val@95 test@95 1</td><td colspan="2"></td><td></td></tr><tr><td>OFA [62]</td><td></td><td>82.2</td><td>82.3</td><td></td><td colspan="2"></td><td colspan="3"></td><td colspan="2"></td><td></td></tr><tr><td>LLaVA-1.5 [35]</td><td>7B</td><td>43.2 45.1</td><td colspan="2"></td><td colspan="3">8.5 9.3</td><td colspan="2">0.3 0.3</td><td colspan="3">17.3 18.2</td></tr><tr><td>LLaVA-NeXT [36]7B</td><td>77.5</td><td colspan="2">77.1</td><td colspan="3">40.7 39.9</td><td colspan="2">1.8 1.7 0.6</td><td colspan="2">40.0</td><td colspan="2">39.6</td></tr><tr><td>LLaVA-OV [28]</td><td>7B 70.8</td><td colspan="2">70.8</td><td colspan="2">23.3</td><td colspan="2">23.6</td><td colspan="2">0.7</td><td colspan="3">31.6 31.7</td></tr><tr><td>Qwen2-VL [61]</td><td>83.3</td><td colspan="2">83.1</td><td colspan="3">72.7 73.0</td><td colspan="2">28.9 27.9</td><td colspan="2">61.6 61.3</td><td colspan="2"></td></tr><tr><td>Perception-R1</td><td colspan="2">2B 2B 85.7</td><td colspan="2">85.4</td><td colspan="3">75.7 76.0</td><td colspan="2">32.1 33.1</td><td colspan="3">64.5 64.8</td></tr></table></body></html>  

<html><body><table><tr><td></td><td></td><td colspan="2"> Edit Distance ↓</td><td colspan="2">F1-score ↑</td><td colspan="2">Precision ↑</td><td colspan="2">Recall ↑</td><td colspan="2">BLEU↑</td><td colspan="2">METEOR↑</td></tr><tr><td></td><td>size</td><td>en</td><td>zh</td><td>en</td><td>zh</td><td>en</td><td>zh</td><td>en</td><td>zh</td><td>en</td><td>zh</td><td>en</td><td>zh</td></tr><tr><td>Nougat [4]</td><td>250M</td><td>25.5</td><td>-</td><td>74.5</td><td></td><td>72.0</td><td></td><td>80.9</td><td></td><td>66.5</td><td></td><td>76.1</td><td></td></tr><tr><td>DocOwl1.5 [23]</td><td>7B</td><td>25.8</td><td></td><td>86.2</td><td></td><td>83.5</td><td></td><td>96.2</td><td></td><td>78.8</td><td></td><td>85.8</td><td></td></tr><tr><td>GOT [65]</td><td>580M</td><td>3.5</td><td>3.8</td><td>97.2</td><td>98.0</td><td>97.1</td><td>98.2</td><td>97.3</td><td>97.8</td><td>94.7</td><td>87.8</td><td>95.8</td><td>93.9</td></tr><tr><td>Qwen2-VL [61]</td><td>2B</td><td>8.0</td><td>10.0</td><td>94.4</td><td>93.0</td><td>96.9</td><td>96.1</td><td>93.0</td><td>90.5</td><td>90.9</td><td>78.0</td><td>94.1</td><td>87.2</td></tr><tr><td>LLaVA-NeXT [36]</td><td>7B</td><td>43.0</td><td>=</td><td>64.7</td><td>1</td><td>57.3</td><td></td><td>88.1</td><td></td><td>47.8</td><td></td><td>58.2</td><td></td></tr><tr><td>Perception-R1</td><td>2B</td><td>3.5</td><td>9.0</td><td>98.2</td><td>94.4</td><td>98.6</td><td>96.3</td><td>97.8</td><td>92.7</td><td>96.7</td><td>74.6</td><td>98.1</td><td>88.9</td></tr></table></body></html>

Table 2: PageOCR evaluation, compared with various strong expert and general models. "en" means English and "zh" means Chinese.  

# 5 Experiments  

The experimental section evaluates Perception-R1’s performance on visual perception tasks (§ 5.1), followed by analytical experiments exploring reinforcement learning (RL)’s role in perception policy learning $(\S5.2)$ . Finally, it discusses the interplay between visual perception and RL, along with key insights for perception policy learning $(\S\ S.3)$ .  

# 5.1 Performance Landscape in Perception Tasks  

We evaluate Perception-R1 on mainstream perception tasks: visual grounding, counting, OCR, and object detection. Experiments use the datasets described in $\S\ 4.3$ and benchmarks for image understanding. Results are in Tables 1–4. See Appendix A.2 for details.  

Visual Grounding is a task that involves localizing visual objects based on linguistic descriptions. Specifically, given a language prompt, the model is required to output the spatial coordinates of the subject (typically a single entity) described in the prompt. As shown in Table 1, we evaluate  

<html><body><table><tr><td colspan="4">Viusal Counting</td></tr><tr><td>method</td><td>size</td><td>Pixm0val</td><td>PixmOtest</td></tr><tr><td>LLaVA-1.5 [35]</td><td>7B</td><td>33.3</td><td>31.0</td></tr><tr><td>LLaVA-1.6 [58]</td><td>7B</td><td>32.7</td><td>31.9</td></tr><tr><td>LLaVA-OV [28]</td><td>7B</td><td>55.8</td><td>53.7</td></tr><tr><td>Qwen2-VL [61]</td><td>2B</td><td>60.2</td><td>50.5</td></tr><tr><td>Perception-R1</td><td>2B</td><td>78.1</td><td>75.6</td></tr></table></body></html>

(a) Visual counting evaluation on Pixmo-Count [13] val set and test set.  

<html><body><table><tr><td></td><td colspan="4">Object Detection</td></tr><tr><td>method</td><td>size epoch</td><td></td><td>AP AP50</td><td>AP75</td></tr><tr><td>YOL0v3[51]</td><td>273</td><td>27.9</td><td>49.2</td><td>28.3</td></tr><tr><td>Faster-RCNN [52] -</td><td>12</td><td>35.6</td><td>55.7</td><td>37.9</td></tr><tr><td>DETR [6]</td><td>41M 500</td><td>42.0</td><td>62.4</td><td>44.2</td></tr><tr><td>Qwen2.5-VL [3]</td><td>3B 1</td><td>16.1</td><td>23.7</td><td>16.7</td></tr><tr><td>Perception-R1'</td><td>3B 1</td><td>31.9</td><td>46.7</td><td>33.4</td></tr></table></body></html>

(b) Object detection evaluation on COCO2017 [32] validation set.  

Table 3: Mainstream visual tasks evaluation including (a) visual object counting and (b) challenging general object detection. Notably, the results of expert model in (b) are copied from MMDetection [7]. † means Perception-R1 for object detection is build based on Qwen2.5-VL-3B-Instruct [3].   
Table 4: General image understanding and reasoning evaluation, compared with various baselines. We select 8 mainstream multimodal benchamrks, i.e., MMBench [38], MMVet [72], MMStar [9], ScienceQA [53], SeedBench [18], MME [16], LLaVA-Bench [37], and ai2D [26] for the comprehensive understanding. We use the model after RL training in the counting tasks for the eval.   


<html><body><table><tr><td></td><td colspan="6">MMBenchMMVetMMStarScienceQASeedBench</td><td colspan="2">MME</td><td colspan="2">LLaVA-Bench AI2D</td></tr><tr><td></td><td>llm</td><td>Avg</td><td>Avg</td><td>Avg</td><td>Avg</td><td>Avg</td><td>Cognition Perception</td><td></td><td>Avg</td><td>Avg</td></tr><tr><td>LLaVA1.5 [35]</td><td>Vicunal.5-7B</td><td>62.8</td><td>32.8</td><td>32.6</td><td>65.4</td><td>60.1</td><td>302.1</td><td>1338.3</td><td>52.6</td><td>51.9</td></tr><tr><td>LLaVA-NeXT [36] Vicuna1.5-7B</td><td></td><td>66.0</td><td>37.9</td><td>37.7</td><td>68.2</td><td>69.1</td><td>195.7</td><td>1419.5</td><td>52.7</td><td>67.4</td></tr><tr><td>Qwen2-VL [61]</td><td>Qwen2-2B</td><td>71.9</td><td>45.6</td><td>46.3</td><td>74.0</td><td>72.7</td><td>418.5</td><td>1471.1</td><td>46.5</td><td>71.6</td></tr><tr><td>Perception-R1</td><td>Qwen2-2B</td><td>71.8</td><td>48.9</td><td>45.7</td><td>73.4</td><td>73.0</td><td>430.0</td><td>1473.9</td><td>58.2</td><td>71.8</td></tr></table></body></html>  

Perception-R1 on three mainstream benchmarks, refCOCO $/+/\mathrm{\Deltag}$ , and report $\operatorname{Acc}@0.5$ , $\operatorname{Acc}@0.75$ , and $\operatorname{Acc}@0.95$ to comprehensively assess its visual grounding capability. We surprisingly find that several SoTA MLLMs exhibit poor performance on the more challenging Acc $\ @0.95$ metric, with scores even below $1\%$ . In contrast, Perception-R1 achieves a stable performance of over $30\%$ on this metric. This observation suggests that the community should prioritize reporting more discriminative results in future evaluations. The experimental results demonstrate that Perception-R1 exhibits strong competitiveness compared to both specialized and general-purpose models.  

Optical Character Recognition (OCR) represents a critical task in visual perception due to its substantial practical value. Current methodologies predominantly adopt either expert models or fine-tuned generalist models for OCR. Perception-R1 pioneers the utilization of RL to further unlock the OCR capabilities of MLLM. As shown in Table 2, our proposed Perception-R1 achieves SoTA performance on the highly challenging OCR benchmark, i.e., PageOCR [34], demonstrating significant superiority over existing expert models, e.g., GOT (98.1 vs. 97.2 F1-score) and robust generalist models, e.g., LLaVA-NeXT (98.1 vs. 64.7 F1-score). Notably, Perception-R1 does not use the Chinese OCR data for training so it is a zero-shot performance for Chinese metric. This breakthrough substantiates the formidable potential of RL applications in OCR tasks, establishing new frontiers for enhancing textual understanding and recognition in complex visual environments.  

Visual Counting, as a fundamental vision task, necessitates models to accurately quantify categoryspecific instances within images, requiring robust visual logic to identify and enumerate targets through structured recognition patterns. In Perception-R1, we adopt a detect-then-count paradigm that reformulates the counting problem into a point detection process. As shown in Table 3a, Perception-R1 achieves remarkable counting performance, surpassing the current strong baselines by a substantial margin $(\mathbf{17.9\%}$ improvement compared to Qwen2-VL in Pixmo val set). This advancement substantiates that RL effectively stimulates models to explore intrinsic visual logic mechanisms (Although counting yields deterministic results, the sequence of counting can exhibit distinct patterns.), thereby enhancing their capacity to resolve complex vision tasks.  

General Object Detection, widely regarded as the crown jewel of computer vision tasks, has long been considered one of the most challenging problems in visual perception. As a pioneering endeavor to integrate RL into object detection, Perception-R1 achieves a groundbreaking milestone, serving as the first pure MLLM to surpass the $\mathbf{30+}$ AP threshold, i.e., 31.9 AP in Table 3b, on the COCO 2017 val set, matching or even exceeding the performance of specialized expert models. This achievement underscores rule-based RL’s immense potential in addressing complex vision tasks requiring sophisticated visual-logic integration.  

Table 5: Ablation Study of Perception-R1. We perform ablation studies to investigate key properties of Perception-R1 across a range of visual perception tasks. Specifically, we report the $\operatorname{Acc}@0.5$ for RefCOCO $/+/\mathrm{\Deltag}$ val set, the F1-score for PageOCR, the average scores for Pixmo-Count, and the AP metric for COCO2017 val set. w/o means without. Notably, there is no reward matching applied to visual grounding and OCR tasks, as these tasks do not involve the multi-subject reward.   


<html><body><table><tr><td rowspan="2">case</td><td colspan="3">Visual Grounding</td><td rowspan="2">OCR PageOCR</td><td colspan="2">Visual Counting</td><td rowspan="2">Detection COC02017</td></tr><tr><td>RefCOCO</td><td>RefCOCO+</td><td>RefCOCOg</td><td>Pixmoval</td><td>PixmOtest</td></tr><tr><td>Perception-R1</td><td>89.1</td><td>81.7</td><td>85.7</td><td>98.4</td><td>78.1</td><td>75.6</td><td>31.9</td></tr><tr><td>w/o reward matching</td><td></td><td></td><td>-</td><td></td><td>77.1</td><td>75.4</td><td>23.5</td></tr><tr><td>w/o RL</td><td>86.8</td><td>77.1</td><td>83.3</td><td>94.4</td><td>60.2</td><td>50.5</td><td>16.1</td></tr><tr><td>w thinking</td><td>75.1</td><td>67.9</td><td>71.3</td><td>77.3</td><td>74.9</td><td>72.8</td><td>25.7</td></tr><tr><td>w/o thinking</td><td>89.1</td><td>81.7</td><td>85.7</td><td>95.7</td><td>78.1</td><td>75.6</td><td>28.1</td></tr><tr><td>RL only</td><td>89.1</td><td>81.7</td><td>85.7</td><td>95.7</td><td>78.1</td><td>75.6</td><td>31.9</td></tr><tr><td>SFT only</td><td>88.2</td><td>80.7</td><td>84.6</td><td>95.3</td><td>58.0</td><td>59.9</td><td>25.9</td></tr><tr><td>SFT+RL</td><td>88.4</td><td>80.7</td><td>85.1</td><td>97.3</td><td>77.1</td><td>75.4</td><td>30.8</td></tr></table></body></html>  

<html><body><table><tr><td>reward function</td><td colspan="3">COC0201 AP AP50</td></tr><tr><td>format reward</td><td></td><td></td><td>AP75</td></tr><tr><td>format reward + location reward (IoU)</td><td>18.8</td><td>25.3</td><td>20.1</td></tr><tr><td>formatreward+locationreward(IoU)+clsreward</td><td>20.2</td><td></td><td>21.4</td></tr><tr><td>format reward + location reward (IoU) + cls reward + recall reward (F1)</td><td>27.6</td><td>27.3 42.0</td><td>28.7</td></tr><tr><td>format reward + location reward (IoU) + cls reward + recall reward (F1) + missing reward</td><td>28.1</td><td>42.0</td><td>29.6</td></tr></table></body></html>  

Table 6: Reward design analysis of Perception-R1. cls reward indicates binary classification reward and missing reward is a penalty to penalize missed detections. To facilitate rapid experimentation, we randomly sampled 10k data from COCO2017 train set for this experiment.  

General Visual Comprehension extends beyond pure perceptual tasks, and we evaluate PerceptionR1 on multiple multimodal benchmarks. As shown in Table 4, we observe an intriguing phenomenon that models trained with RL for vision-specific tasks, e.g., counting task, exhibit concurrent performance gains in generic comprehension benchmarks. We attribute this cross-task enhancement to the perception policy learning, which drives the model to discover superior image interpretation patterns.  

# 5.2 Ablation Study of Perception-R1  

In this section, we aim to conduct a comprehensive ablation study to systematically investigate the contributions of critical components within Perception-R1. Experimental results are shown in Table 5. From the experimental results, we can derive three principal empirical findings:  

Reward matching enhances the explorability of multi-subject visual perception. As evidenced by the comparative results between row 1 and 2 in Table 5, replacing the bipartitle matching with sequential matching leads to substantial performance degradation in both visual counting and object detection task. This suggests that sequential matching constrains the RL exploration space. On the contrast, the bipartite matching mechanism provides more possibility in reward assignment, enabling the model to explore optimal visual perception patterns.  

Explicit thinking processes prove non-essential for contemporary visual perception. Comparative analysis of row 3 and 4 reveals consistent performance degradation across all four evaluated perception tasks when incorporating an explicit thinking process during both training and inference phases. Similar phenomenon also emerges in image classification tasks [30]. We posit that this phenomenon arises because current visual perception tasks are more oriented toward visual logic rather than semantic logic. This shift implies that explicit language-centered reasoning processes are unnecessary, as models tend to focus more on learning implicit visual patterns.  

Perceptual perplexity determines RL superiority over SFT. We compare the different combinations of post-training method, i.e., SFT, RL, and $\mathrm{SFT+RL}$ , across four perception tasks, as shown in row 6, 7, 8 of Table 5. In tasks with high perceptual perplexity, such as counting and detection (multiple objects and categories), RL demonstrates superior performance enhancement compared to SFT or even $\mathrm{SFT+RL}$ . Conversely, in low-perplexity tasks such as grounding and OCR, RL underperforms relative to SFT or $\mathrm{SFT+RL}$ . This indicates that high perceptual perplexity a significant factor influencing the effectiveness of RL. It suggests that RL techniques should be applied to tasks with greater perceptual perplexity, where the exploration space for perception policy is larger.  

Figure 2: Scalability analysis of Perception-R1. We select two primary tasks: grounding and counting. We visualize the training reward curves under varying numbers of rollouts and evaluate the final performance of each task. All experiments are conducted with $5k$ sampled data. And the default rollout number setting $(1\times)$ is 8.  

# 5.3 More In-depth Analysis  

In this section, we explore several key properties of Perception-R1 to further enhance our understanding of Perception Policy Learning with RL.  

Analysis of reward design for perception policy learning. We introduced the details of reward function of Perception-R1 in $\S4.3$ . In this part, we examine the influence of these reward functions on perception policy learning. Specifically, using object detection as a case study, we incrementally integrate the designed answer reward into the format reward, as illustrated in Table 6. The results indicate that the progressive introduction of refined reward functions leads to consistent improvements in detection performance, ultimately exceeding the performance of expert models. This underscores the critical role of reward design in perception policy learning. Furthermore, it identifies a promising avenue for future research: the development of more refined and task-specific reward functions to enhance perception policy learning.  

Analysis of scaling up rollout for perception policy learning. The scalability of RL is a key concern of existing LLM post-training. In this part, we analyze the scalability of Perception-R1, focusing specifically on scaling up the number of rollouts. As shown in Figure 2, we conduct rollout-scaling experiments in two tasks: visual grounding and visual counting. The results indicate that increasing rollout count enhances reward optimization and final performance. This demonstrates Perception-R1’s strong scaling properties and underscores the critical role of rollout quantity in scaling perception policies. By generating sufficient rollouts, the model broadens its exploration space, increasing the diversity of candidate solutions for reward evaluation. This expansion accelerates convergence to optimal visual perception patterns.  

# 6 Limitation and Conclusion  

"What can RL bring to MLLM?" is a public question since the propose of DeepSeek-R1. Several latest works attempt to apply RL from the perspective of language-centric visual reasoning [39, 15, 41]. However, in this paper, we take a different pathway and argue that perception is a crucial prerequisite for visual reasoning. Only by fully unlocking the perception patterns of MLLMs can the models possess the ability to reason about complex visual tasks. Nevertheless, we regrettably find that many current perception tasks are overly simplistic, which limits the exploration space for RL. This, in turn, restricts the possibility of MLLMs achieving a perceptual "Aha moment" through thinking process. Finding more appropriate perception tasks, aka., meta task, may be the key to addressing this issue.  

In a summary, this work takes a pioneering step in exploring the potential of rule-based RL in MLLM post-training for perception policy learning. Through extensive experimental analysis, we establish several valuable cognition about perception policy learning with RL. Driven by these findings, we build Perception-R1, a simple, effective, and scalable RL framework for efficient perception policy learning. Perception-R1 sets new SoTAs across multiple visual perception tasks, particularly in object detection tasks. By introducing a novel paradigm, it achieves and even surpasses the performance of expert models, thereby demonstrating the significant potential of perception policy learning.  

References   
[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.   
[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.   
[3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025.   
[4] Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural optical understanding for academic documents. arXiv preprint arXiv:2308.13418, 2023.   
[5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-languageaction models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023.   
[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213–229. Springer, 2020.   
[7] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.   
[8] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $\$3$ . https://github.com/Deep-Agent/ R1-V, 2025.   
[9] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024.   
[10] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: A comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025.   
[11] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024.   
[12] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun,  

T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.  

[13] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024.   
[14] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi. DreamLLM: Synergistic multimodal comprehension and creation. In The Twelfth International Conference on Learning Representations, 2024.   
[15] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025.   
[16] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023.   
[17] Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, and Lijie Wen. Interpretable contrastive monte carlo tree search reasoning, 2024.   
[18] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023.   
[19] GPT-4o. Hello gpt-4o, 2024.   
[20] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961–2969, 2017.   
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.   
[22] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: A visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14281–14290, 2024.   
[23] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, et al. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. arXiv preprint arXiv:2403.12895, 2024.   
[24] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang. Open-reasoner-zero: An open source approach to scaling reinforcement learning on the base model. https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero, 2025.   
[25] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1780–1790, 2021.   
[26] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is worth a dozen images. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14, pages 235–251. Springer, 2016.   
[27] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83–97, 1955.   
[28] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024.   
[29] Jinyang Li, En Yu, Sijia Chen, and Wenbing Tao. Ovtr: End-to-end open-vocabulary multiple object tracking with transformer. arXiv preprint arXiv:2503.10616, 2025.   
[30] Ming Li, Shitian Zhao, Jike Zhong, Yuxiang Lai, and Kaipeng Zhang. Cls-rl: Image classification with rule-based reinforcement learning. arXiv preprint arXiv:2503.16188, 2025.   
[31] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step, 2023.   
[32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014.   
[33] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024.   
[34] Chenglong Liu, Haoran Wei, Jinyue Chen, Lingyu Kong, Zheng Ge, Zining Zhu, Liang Zhao, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Focus anywhere for fine-grained multi-page document understanding. arXiv preprint arXiv:2405.14295, 2024.   
[35] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26296–26306, 2024.   
[36] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.   
[37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.   
[38] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023.   
[39] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025.   
[40] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11–20, 2016.   
[41] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025.  

[42] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025.  

[43] OpenAI. Chatgpt. https://openai.com/blog/chatgpt, 2022.   
[44] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   
[45] OpenAI. Learning to reason with llms, September 2024.   
[46] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730–27744, 2022.   
[47] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.   
[48] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: A human-aligned benchmark for personalized image generation. arXiv preprint arXiv:2406.16855, 2024.   
[49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021.   
[50] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.   
[51] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.   
[52] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE transactions on pattern analysis and machine intelligence, 39(6):1137–1149, 2016.   
[53] Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. Scienceqa: A novel resource for question answering on scholarly articles. International Journal on Digital Libraries, 23(3):289–301, 2022.   
[54] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   
[55] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.   
[56] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.   
[57] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025.   
[58] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024.   
[59] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   
[60] Fei Wang, Wenxuan Zhou, James Y Huang, Nan Xu, Sheng Zhang, Hoifung Poon, and Muhao Chen. mdpo: Conditional preference optimization for multimodal large language models. arXiv preprint arXiv:2406.11839, 2024.   
[61] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024.   
[62] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In International conference on machine learning, pages 23318–23340. PMLR, 2022.   
[63] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Vary: Scaling up the vision vocabulary for large visionlanguage model. In European Conference on Computer Vision, pages 408–424. Springer, 2024.   
[64] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, En Yu, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Small language model meets with reinforced vision vocabulary. arXiv preprint arXiv:2401.12503, 2024.   
[65] Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, et al. General ocr theory: Towards ocr-2.0 via a unified end-to-end model. arXiv preprint arXiv:2409.01704, 2024.   
[66] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824–24837, 2022.   
[67] Huajian Xin, Z. Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, Wenjun Gao, Qihao Zhu, Dejian Yang, Zhibin Gou, Z. F. Wu, Fuli Luo, and Chong Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024.   
[68] En Yu, Kangheng Lin, Liang Zhao, Yana Wei, Zining Zhu, Haoran Wei, Jianjian Sun, Zheng Ge, Xiangyu Zhang, Jingyu Wang, et al. Unhackable temporal rewarding for scalable video mllms. arXiv preprint arXiv:2502.12081, 2025.   
[69] En Yu, Tiancai Wang, Zhuoling Li, Yuang Zhang, Xiangyu Zhang, and Wenbing Tao. Motrv3: Release-fetch supervision for end-to-end multi-object tracking. arXiv preprint arXiv:2305.14298, 2023.   
[70] En Yu, Liang Zhao, Yana Wei, Jinrong Yang, Dongming Wu, Lingyu Kong, Haoran Wei, Tiancai Wang, Zheng Ge, Xiangyu Zhang, et al. Merlin: Empowering multimodal llms with foresight minds. arXiv preprint arXiv:2312.00589, 2023.   
[71] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 69–85. Springer, 2016.   
[72] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023.   
[73] Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong, Chunrui Han, et al. Chatspot: Bootstrapping multimodal llms via precise referring instruction tuning. arXiv preprint arXiv:2307.09474, 2023.   
[74] Zining Zhu, Liang Zhao, Kangheng Lin, Jinze Yang, En Yu, Chenglong Liu, Haoran Wei, Jianjian Sun, Zheng Ge, and Xiangyu Zhang. Perpo: Perceptual preference optimization via discriminative rewarding. arXiv preprint arXiv:2502.04371, 2025.  

# A Appendix  

In this appendix, we provide additional details about Perception-R1, which are omitted due to the 9-page limit of the main paper. Specifically, Section A.1 elaborates on the detailed dataset and training settings. Section A.2 presents more experimental results.  

# A.1 Additional Details about Experimental Setting  

More detailed dataset information of Perception-R1. In Section 4.3, we introduced what data was used for RL post-training of Perception-R1 on which tasks. In this part, we will provide more detailed information about the datasets, as shown in Table 7.  

<html><body><table><tr><td>tasks</td><td>datasets</td><td>Original</td><td>Used</td><td>Ratio</td></tr><tr><td>visual grounding</td><td>RefCOCO/RefCOCO+/RefCOCOg</td><td>320k</td><td>5k</td><td>1.56%</td></tr><tr><td rowspan="3">OCR visual counting</td><td>PageOCR</td><td>50k</td><td>5k</td><td>10%</td></tr><tr><td>PixMo-Count</td><td>1.9M</td><td>10k</td><td>0.5%</td></tr><tr><td>COCO2017</td><td>110k</td><td>110k</td><td>100%</td></tr><tr><td>object detection overall</td><td></td><td>2.38M</td><td>130k</td><td></td></tr></table></body></html>  

Table 7: Training dataset statistics. Notably, we do not mix the data from different perception tasks for joint training because the rewards for different tasks vary.  

More detailed training setting information of Perception-R1. Section 4.3 elaborates on several key parameters of Perception-R1. In this part, we further demonstrate the diverse prompts employed for distinct perception tasks, as shown in Table 8.  

<html><body><table><tr><td>tasks</td><td>system prompt</td><td>user prompt</td></tr><tr><td>visual grounding</td><td>Qwen2-VL</td><td>Output the bounding box of the question} in the image.</td></tr><tr><td>OCR</td><td>Qwen2-VL</td><td>OCR this image.</td></tr><tr><td>visual counting</td><td>Qwen2-VL</td><td>Output all the bounding boxes of the {label}</td></tr><tr><td>object detection</td><td>Qwen2.5-VL</td><td>Please output bbox coordinates and names of {90 categories of COCO}.</td></tr></table></body></html>  

Table 8: Prompts of Perception-R1. The system prompt of Perception-R1 follows Qwen2-VL [61] and Qwen2.5-VL [3].  

# A.2 Additional Experimental Results  

In this section, we provide more qualitative analysisi of Perception-R1 on multiple visual perception tasks. The selected cases are shown in Figure 3-6.  

Question: <image>Output all the bounding boxes of the bird.  

Figure 3: Demo case of Percpetion-R1 on visual counting task.  

# Perception-R1:  

(259,427),(466,591)(298,172),(497,315)(558,344),(759,524)(0,273),(61,466)(30,790),(222,875)(27,9),(208,207)(0,902), (95,998)(0,637),(51,714)(15,389),(215,599) Num boxes = 9  

Input:  

# Question:  

<image>OCR this image.  

# Perception-R1:  

G E A R L A $\mathbf{B}\backslash\mathrm{n}\backslash\mathrm{nRiding~Dirty}\backslash\mathrm{n}\backslash\mathrm{nA}$ muddy mix of road- and mountain-bike racing, cyclocross doubles the thrill of both sports. Here’s the gear to get you started. $\backslash\mathrm{n}\backslash\mathrm{nby}$ Berne Broudy\n\nCyclocross is as cool as it looks: Racers pedal drop-handlebar bikes with trail-ready tires through a course that often includes pavement, dirt, mud, and grass, as well as obstacles that force you to dismount and sling your bike over your shoulder. “All you really need is to be tough and have a good attitude and a desire to suffer a little bit,” says Stu Thorne, founder and director of the Cannondale professional cyclocross team. The right gear helps, too. $\scriptstyle\backslash\mathrm{n}\backslash\mathrm{nBest}$ for Entry Level\n\nCannondale CAADX $105\backslash\mathrm{n\backslashnWhat}$ you sacrifice with extra weight you make up for with a highly capable aluminum frame that’ll keep you going between races. A carbon-blade fork helps eat up some of the pounding the courses can dish out. The dependable 2x11 speed Shimano 105 drivetrain powers through any grade, and TRP cable disc brakes perform well even in mud. Meanwhile, the 35cc tires and stable geometry make the CAADX a superfun commuter if you’re looking for a bike that can multitask.\n\ncannondale.com $\$1,970\backslash$ for Experienced Cyclists\n\nSpecialized CruX Elite $\mathrm{X1}\backslash\mathrm{n}\backslash\mathrm{nIf}$ you already log long miles on a premium road bike, this should be your new cross rig. The carbon frame is as light as it is. $\scriptstyle\operatorname{\backslashn\backslashnA}$ low bottom bracket and relatively slack head angle, which make it stable, helped us sail through rocks and roots and corner quickly. The tires can be run tubeless to better resist flats, and extra clearance means they spin freely when caked with mud. The CruX Elite is playful and fast — and something you won’t outgrow as you collect medals. $\ln\backslash\backslash\mathrm{nspecialized.com}\ \&\ 3,000\backslash\mathrm{nACCESSORIES}\backslash\mathrm{n}\backslash\mathrm{nCr}$ aft Shield Glove $\scriptstyle\backslash{\mathrm{n}}\backslash{\mathrm{nThe}}$ cross season typically runs from September through February, so you’ll need hearty gloves like these, with a fleece lining and a waterproof base, for warmth on wet race days. craftsports.us $\$78\backslash\mathrm{n D a r n}$ Tough Micro Crew\n\nUnlike other bike races, cyclocross requires you to be on foot at times. So light, strong socks are key. These aren’t likely to wear out, but Darn Tough will replace them if they do. darntough.com $\$18\backslash\mathrm{n\backslash n P a r k}$ Tool Brush Set\n\nThe mud, dirt, and grime that builds up during off-road rides can damage key components. This kit does more than just keep your bike looking fresh; it keeps it healthy, too. parktool.com $\mathfrak{$8o}\backslash\mathrm{{n}\backslash\mathrm{{}}}$ \nRapha Arm and Leg Warmer\n\nThese merino layers, which have a bit of Lycra for stretch, peel off easily when the weather warms up. And they dry quickly, whether you sweat profusely or get caught in a sudden squall. rapha.cc From $\$70$ SmartGauge ${\bf D2\backslash n\backslash n F o r}$ peak performance, adjust your tire pressure to suit the terrain. (On soft trails, lower pressure makes it grip better.) The SmartGauge makes it a snap with a fast, readable result. topeak.com $\$40$  

Figure 4: Demo case of Percpetion-R1 on OCR task.  

# Question:  

<image>Output the bounding box of the woman holding a child in the image.  

Figure 5: Demo case of Percpetion-R1 on visual grounding task.  

# Perception-R1:  

The woman holding a child(157,459),(208,622)  

# Question:  

<image>Please output bbox coordinates and names of {90 categories of $\operatorname{COCO}{\}$ .  

# Perception-R1:  

Figure 6: Demo case of Percpetion-R1 on general object detection task. The color of bounding boxes correspond to the category.  

[ {"bbox_2d": [435, 1, 616, 415], "label": "person"}, {"bbox_2d": [218, 112, 323, 253], "label": "person"}, {"bbox_2d": [147, 165, 225, 280], "label": "person"}, {"bbox_2d": [183, 126, 239, 242], "label": "person"}, {"bbox_2d": [276, 77, 338, 192], "label": "person"}, {"bbox_2d": [127, 54, 471, 406], "label": "laptop"}, {"bbox_2d": [108, 174, 156, 243], "label": "cell phone"}, {"bbox_2d": [26, 107, 103, 214], "label": "potted plant"}, {"bbox_2d": [64, 29, 280, 137], "label": "chair"} ]  