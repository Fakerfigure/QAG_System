# SciQAG: A Framework for Auto-Generated Science Question Answering Dataset with Fine-grained Evaluation  

Yuwei Wan1,2, Yixuan $\mathbf{Liu}^{1}$ , Aswathy Ajith3, Clara Grazian4,5, Bram Hoex6, Wenjie Zhang7, Chunyu Kit2,\*, Tong Xie1,6,\*, and Ian Foster3,\*  

1GreenDynamics Pty. Ltd, Kensington, NSW, Australia $^2$ Department of Linguistics and Translation, City University of Hong Kong, Hong Kong, China 3Department of Computer Science, University of Chicago, Chicago, IL, USA 4School of Mathematics and Statistics, University of Sydney, Camperdown, NSW, Australia 5DARE ARC Training Centre in Data Analytics for Resources and Environments, Australia 6School of Photovoltaic and Renewable Energy Engineering, University of New South Wales, Kensington, NSW, Australia 7School of Computer Science and Engineering, University of New South Wales, Kensington, NSW, Australia \*Corresponding author: tong.xie@unsw.edu.au, foster@cs.uchicago.edu, ctckit@cityu.edu.hk  

# Abstract  

We introduce SciQAG, a novel framework for automatically generating high-quality science question-answer pairs from a large corpus of scientific literature based on large language models (LLMs). SciQAG consists of a QA generator and a QA evaluator, which work together to extract diverse and research-level questions and answers from scientific papers. Utilizing this framework, we construct a large-scale, high-quality, open-ended science QA dataset containing 188,042 QA pairs extracted from 22,743 scientific papers across 24 scientific domains. We also introduce SciQAG-24D, a new benchmark task designed to evaluate the science question-answering ability of LLMs. Extensive experiments demonstrate that finetuning LLMs on the SciQAG dataset significantly improves their performance on both open-ended question answering and scientific tasks. To foster research and collaboration, we make the datasets, models, and evaluation codes publicly available, contributing to the advancement of science question answering and developing more interpretable and reasoningcapable AI systems.  

# 1 Introduction  

Question-answering (QA) datasets have been instrumental in advancing natural language processing (NLP) research by enabling the development of models that can understand and reason about textual information. These datasets encompass various domains and question types, such as reading comprehension (Hermann et al., 2015), open-domain QA (Chen et al., 2017; Longpre et al., 2021), and visual QA (Schwenk et al., 2022; Masry et al., 2022). They serve as benchmarks for evaluating the performance of QA systems and drive the creation of more sophisticated models (Chen et al., 2017; Rajpurkar et al., 2016). Among these, science QA datasets such as SciQ (Welbl et al., 2017) and SciencQA (Saikh et al., 2022) have garnered significant attention due to their potential to assess models’ ability to comprehend and apply scientific knowledge (Clark et al., 2018; Mihaylov et al., 2018). The importance of these datasets lies not only in their capacity to test models’ understanding of scientific concepts but also in their ability to evaluate the models’ reasoning capabilities when dealing with complex relationships and drawing conclusions based on experimental evidence.  

Figure 1: The SciQAG framework for generating science QA from the scientific literature. The dashed line represents optional fine-tuning.  

The advent of large language models (LLMs) like GPT-4 (OpenAI et al., 2023), LLaMA-3 (Touvron et al., 2023b), PaLM (Chowdhery et al., 2022), marks a significant milestone in the pursuit of general artificial intelligence. These models have proven instrumental in various aspects, including generation, annotation, and paraphrasing (Yoo et al., 2021; Xie et al., 2024; Sahu et al., 2022; Xie et al., 2023b; Wang et al., 2023; Tang et al., 2023). Besides general applications,this trend has permeated the natural sciences, giving rise to domainspecific models like BioBERT (Lee et al., 2020), Med-PALM (Singhal et al., 2022), and Galactica (Taylor et al., 2022). While these models may not always necessitate pre-training from scratch, it often involves fine-tuning on domain-specific data. QA pairs are essential for fine-tuning and evaluating LLMs (Kamalloo et al., 2023; Rasool et al., 2024), particularly when domain-specific questionanswering capabilities are desired. For example, (Zhang et al., 2024a) fine-tuning data comprises 7M instruction tuning QA pairs, with the objective of instruction fine-tuning often being causal language modeling in GPT-based scientific LLMs (Zhang et al., 2024b). As such, the development of more comprehensive science QA datasets and related evaluation methods is a practical necessity for advancing scientific LLMs.  

Despite the progress made with existing science QA datasets, several challenges persist. First, these datasets often lack variety in question forms, with most being multiple-choice questions (MCQ). Although the performance of MCQs is established by exact match and conclusions obtained from such evaluation tend to be more robust (Subramanian et al., 2024), they inadequately test the language and logical reasoning capabilities of LLMs, necessitating more challenging QA benchmarks. Second, the content of these datasets is primarily sourced from textbooks, resulting in an insufficient difficulty level to assess the high-level knowledge retention of LLMs in real-world academic research or industrial production settings. Third, the creation of these benchmarks often relies on crowd-sourcing or expert-driven approaches, which can be costly (Lee et al., 2023a; Xie et al., 2023c) due to the need for specialized skills (Xie et al., $2023\mathrm{a}$ ; Ye et al., 2024). Addressing these challenges is crucial for developing robust and comprehensive QA datasets that can more accurately evaluate the capabilities of scientific LLMs.  

To tackle these challenges, we focus on the task of open-ended, closed-book science QA in this paper. Open-ended questions do not have predetermined or fixed answers, while closed-book indicates that no external knowledge or context is provided with the question (Roberts et al., 2020).  

We introduce Automatic Generation of Science Question Answering (SciQAG), a framework for automatically generating QA pairs from scientific articles (see Figure 1). SciQAG comprises two main components: a QA generator and a QA evaluator. The QA generator leverages LLMs to extract relevant information from scientific papers and generate diverse question-answer pairs while the QA evaluator assesses or filters the generated QA pairs. By applying SciQAG to a large corpus of scientific papers, we constructed a high-quality science QA dataset, SciQAG-24D, consisting of 188,042 QA pairs. We compared the zero-shot performance of several LLMs on the SciQAG-24D test set and explored fine-tuning impacts of the training set on various datasets, including unseen QA pairs, SciQ (Welbl et al., 2017), and scientific tasks. The results demonstrate that incorporating science QA data during fine-tuning significantly enhances the model’s ability to understand and apply scientific knowledge, improving performance on various scientific tasks. These findings underscore the value of the SciQAG-24D dataset and highlight the potential of scientific LLMs in advancing scientific research and problem-solving.  

# 2 Related Work  

# 2.1 Science QA benchmarks  

Science QA benchmarks, which typically consist of questions and answers from scientific literature, textbooks, or domain experts, play a crucial role in evaluating the performance of QA systems in scientific domains. Table 1 provides a comparison of several popular science QA benchmarks, highlighting the prevalence of manually collected elementarylevel multiple-choice questions Notable examples includes the work of Welbl et al. (2017), who proposed a novel approach to assist crowd workers in generating MCQs with answers from relevant passages. Lu et al. (2022) compiled a multi-modal multiple-choice QA dataset from elementary and high school science curricula, emphasizing the evaluation of model reasoning skills. In the biomedical domain, Jin et al. (2019) produced a QA dataset based on PubMed article abstracts, focusing on reasoning beyond factoid answer extraction, but limited to yes, no, or maybe answers.However, most previous open-ended long-form QA datasets on scientific literature are limited to domains like AI (Lee et al., 2023b; Dasigi et al., 2021) or biomedical (Otegi et al., 2022; Jin et al., 2021),leaving room for the development of more comprehensive and diverse science QA benchmarks.  

<html><body><table><tr><td>Dataset</td><td>Size</td><td>Method</td><td>Q-type</td><td>Difficulty</td></tr><tr><td>SciQ (Welbl et al., 2017)</td><td>13,679</td><td>Manual</td><td>MCQ</td><td>Element - college</td></tr><tr><td>ScienceQA (Saikh et al., 2022)</td><td>21,208</td><td>Manual</td><td>MCQ</td><td>Element - high</td></tr><tr><td>OpenBookQA (Mihaylov et al., 2018)</td><td>5,957</td><td>Manual</td><td>MCQ</td><td>Element</td></tr><tr><td>ARC (Clark et al., 2018)</td><td>7,787</td><td>Manual</td><td>MCQ</td><td>Element</td></tr><tr><td>MMLU (Hendrycks et al., 2020)</td><td>15,908</td><td>Manual</td><td>MCQ</td><td>Element - profession</td></tr><tr><td>SciQA (Auer et al., 2023)</td><td>2,565</td><td>Automated</td><td>Open-ended</td><td>Profession</td></tr></table></body></html>

Table 1: Overview of several popular science QA benchmark datasets, including their size, construction method, question type, and difficulty level.  

# 2.2 Automatic QA generation  

Automatic question generation has been explored in various contexts, with a primary focus on educational applications. Early methods utilized syntactic transformation templates (Mitkov et al., 2003; Heilman and Smith, 2010)to transform text into questions. Pandya and Bhatt (2021) established a taxonomy for human-generated QA datasets and evaluation strategies for comparing modelgenerated answers with reference answers. However, current automatic question-generation methods typically rely on predefined answers (Mulla and Gharpure, 2023). For example, Virani et al. (2023) introduced a versatile Question Answer Generation (QAG) system, adaptable to various question types and domains, but it still requires predefined answers for question generation. Recently, Auer et al. (2023) used a rule-based method to automatically build SciQA for scholarly knowledge; but this approach relies on an existing knowledge graph. In contrast, the approach presented in this paper aims to generate high-quality open-ended QA pairs from scientific texts without relying on manually curated answers, thus addressing a gap in current literature.  

# 2.3 Evaluation metrics for open-ended QA  

Evaluating the quality of long-form generated text, such as open-ended QA pairs, is a challenging task due to the subjective nature of the assessment (Das et al., 2021). Traditional evaluation metrics, including BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005a), and F1 (Rajpurkar et al., 2016), rely on $n$ -gram between the generated text and a reference text. More recent methods, such as BERTScore (Zhang et al., 2020), sentence mover similarity (Clark et al., 2019), and  

MAUVE (Pillutla et al., 2021) eliminate the need for $n$ -gram similarities, but still require reference texts (Chen et al., 2019). Min et al. (2023) propose an alternative approach that evaluates the factuality of long-form generated text by breaking it down into atomic facts and computing the percentage of atomic facts supported by a reliable knowledge source. The application of large language models (LLMs) to evaluate the quality of generated science QA datasets (Song et al., 2023) has also shown promise. Building upon these advancements, this paper adopts a multi-perspective approach using LLMs to evaluate the quality of open-ended QA pairs, contributing to the development of more comprehensive evaluation metrics for science QA.  

# 3 SciQAG framework  

We propose a cost-effective framework to generating closed-book QA pairs focusing on scientific knowledge from the literature at scale, called SciQAG. The main idea is to train a generator to convert full-text scientific papers into QA pairs and use an evaluator to filter out those that do not meet quality standards.  

# 3.1 QA generator  

The task is defined as follows: given seed input texts $T$ , for each input text $t$ , the generator should firstly generate 15 keywords $k$ that capture the most important terms and concepts in the text, then generate a set $S=\{(q_{i},a_{i})\}_{i=1}^{n}$ focusing on the generated keywords $k$ , where $\forall i\in\{1,...,10\}$ , $q_{i}$ is the question and $a_{i}$ is the answer to $q_{i}$ . To generate $S$ , one should learn a generator $G(S|T;\theta)$ with $\theta$ the model parameters. Thus, given a new input text $\hat{t}$ , following $G(S|T;\theta)$ , one can directly generate a $\hat{S}$ consisting of QA pairs (by firstly generating 15 keywords to guide the QA generation).  

We designed an effective prompt for LLM to generate QA pairs with detailed information extracted from scientific papers. Since our final generated QA dataset is intended to be closed-book, implying that models trained on our dataset will not have access to background knowledge during either training or inference, we also give guidelines to ensure that the generated QA do not depend on or reference the content in the original paper (see Appendix D for prompts).  

For the dataset generation in this study, we chose to fine-tune an open-source LLM as the generator to balance performance and cost (see Section 4.2). However, users of this framework can select any open-source or closed-source LLM of their preference for the generator, either using fine-tuning or prompting.  

# 3.2 QA Evaluator  

Inspired by the evaluation metrics introduced in HoneyBee (Song et al., 2023), we develop a comprehensive evaluation metric, RACAR, which consists of five dimensions: Relevance, Agnosticism, Completeness, Accuracy, and Reasonableness.  

• Relevance quantifies how relevant generated QA pairs are to the information presented in the source article. The generated questions are required to ask about the facts and knowledge provided in the article.   
• Agnosticism: We require that the generated questions must not refer to demonstrative content from the original text, like figures or tables in the provided dataset. This dimension of the metric measures the degree of contextindependence of the generated questions.   
• Completeness assesses whether the answer comprehensively covers all relevant aspects of the question and effectively utilizes the details in the paper.   
• Accuracy measures how well the generated answer corresponds to the relevant facts or information presented in the given paper. Any claims or statements in the answer should be supported by evidence from the paper.   
• Reasonableness pertains to the internal logic within the generated answer. It involves assessing whether the answer is logically coherent without contradictions.  

To measure alignment between GPT-4 and human evaluation, we engaged two domain experts to manually evaluate the 10 QA pairs for 10 articles (a total of 100 pairs) with the same RACAR metric. Figure 2 shows the averaged spearman and pearson correlations between GPT-4 assigned scores and expert-annoted scores.  

Figure 2: Spearman and Pearson correlations between GPT-4 assigned scores and expert-annotated scores.  

Our evaluator serves two purposes: 1) evaluating the quality of the generated QA pairs, and 2) acting as a filter to discard low-quality QA pairs after establishing a standard. It is noteworthy that this standard is customizable; it can be a score threshold or a selection of the top k QA pairs based on their overall ranking. In this study, we directly use GPT-4 as QA evaluator to evaluate the generated QA pairs (see Appendix C for prompts) along these dimensions on a scale of 1–5, with 1 being unacceptable and 5 fully acceptable. Users of this framework can select any open-source or cosedsource LLM for evaluator, either using fine-tuning or prompting.  

# 4 SciQAG-24D Dataset  

# 4.1 Paper collection  

From the Web of Science (WoS) Core Collection database, we obtained scientific papers that met the following criteria: 1) categorized in physical science disciplines (materials science, chemistry, physics, energy); 2) available in full-text HTML/XML format from publishers like Elsevier, Springer Nature, and Royal Society of Chemistry; and 3) written in the English language (see Appendix A for details). We selected 24 WoS categories and for each WoS category, we selected most highly cited papers and employed the SciCrawler paper downloading tool (Xie et al., 2023a) to retrieve their content in plain text format. This dataset of 22,743 papers aims to serve as a credible, rich, balanced, and representative source of scientific knowledge. The number of papers selected for each category can be found in Table 4 in Appendix B.  

Figure 3: Proportion of papers of different categories in the training and testing of the SciQAG-24D dataset.  

# 4.2 Using the SciQAG Framework  

To fine-tune an open-source LLM as QA generator, we first randomly selected 426 papers from the paper collection as input to produce 4260 seed QA pairs by prompting GPT-4 (see Appendix C). Then, we fine-tuned vicuna-7b-v1.5-16k model (Zheng et al., 2023) on seed data. The data employs the instruction schema (Wang et al., 2022) composed of three elements: <instruction>, <input>, and <output>. The seed QA generation prompt was converted into the <instruction>. The seed paper filled the <input> field, and the <output> were the generated seed QA pairs. We concatenated the instruction and instance input as a prompt and train the model to generate the instance output in a standard supervised way. It is instructive to compare the SciQAG science QA generator with the self-questioning prompting approach of Fang et al. (2024), which as we show in Appendix F can also be used to generate factual QA pairs from text.  

Using the trained QA generator, we performed inference on the remaining papers, generating a total of 227,430 QA pairs (including seed QA pairs). We sampled 50 papers from each category (a total of 1,200 papers) and their generated QA pairs, using GPT-4 to compute the RACAR score for each QA pair. QA pairs with any score lower than 3 were filtered out to form a test set. For the remaining QA pairs, a simple rule-based approach was used to remove all pairs containing "this paper" or "this study" to form a training set.  

# 4.3 Statistics of SciQAG-24D  

Overview. Using SciQAG framework, we established our open-ended science QA benchmark dataset, SciQAG-24D, which covers 24 scientific domains. Figure 3 shows the proportion of papers of different categories in the training and testing. And Table 4 in Appendix B lists the categories, the train-test split, the number of papers and generated QAs. The dataset consists of a total of 22,743 original papers, with 21,543 papers used for training set and 1,200 papers for test set. After filtering, the training set includes 21,529 papers and 179,511 QA pairs, while the filtered test set contains 1,199 papers and 8,531 QA pairs. This extensive collection of QA pairs aims to support the development and evaluation of open-ended QA systems in various scientific fields. This dataset has multiple applications: the training set can be used to fine-tune LLMs, infusing them with scientific knowledge; while the test set can be used to evaluate the overall performance of LLMs on open-ended, closedbook science QA tasks, or within specific domains. Given its substantial size, the test set can also serve as high-quality data for fine-tuning. Next, we provide several statistical metrics and visualizations to assess the quality of QA pairs from various perspectives.  

Source validation of numeric values. We encourage the generator to reference data from the source paper when generating answers. However, we are concerned about inevitable hallucination (Xu et al., 2024), wherein the model fabricates numbers as support. Given our requirement that both questions and answers stay within the scope of the source paper, we assume that a numerical value in a generated answer that does not occur in the original text has uncertain authenticity. Thus we define the following ratio, which ideally should have value 1:  

# numeric values in answers that appear in paper  

We find that $27.29\%$ of generated answers contain numerical information. Of the numeric values found in answers, $99.15\%$ occur in the source text, suggesting that fabricated numerical data are rare.  

Diversity of questions: Since we aim for unique questions that are not mere paraphrases of each other, we calculated the text similarity between generated questions. Traditional metrics like METEOR (Banerjee and Lavie, 2005b) fail to capture the true intent behind textually similar questions, as seen in the example:  

• What are the advantages of using zeolites? • What are the disadvantages of using zeolites?  

To address this, we use chain-of-thought (CoT) (Wei et al., 2022) prompting with the Mixtral ${8\times7}\mathrm{B}$ model (Jiang et al., 2024) in a few-shot setting (see Appendix E). It instructs the model to predict similarity scores by: 1) understanding the purpose; 2) identifying aspects; 3) comparing intent; and 4) assigning a score. We computed pairwise similarities for sets of 10 questions generated for each of 50 papers. Most questions exhibit dissimilarity (see Figure 4). The distribution of the 2250 similarity scores, calculated from the $50{\times}10{\times}9/2$ possible question pairs, reveals that $87.29\%$ of the scores fall below 0.3, while only $2.27\%$ exceed 0.7. The average similarity score of 0.186 demonstrates the effectiveness of the SciQAG method in generating a diverse set of questions with minimal overlap, ensuring that the generated questions are distinct from one another.  

Coverage rate of answers. To show our answers are from different parts of the paper, we measure the coverage rate of a paper’s associated set of answers $A$ . We first calculate the cosine similarity of each answer $a\in A$ to each sentence $s$ in the paper and then construct as $S_{15}$ the set of sentences in the paper that are in the topk $(k=15\%)$ similarity scores for any answer in $A$ . We then divide the paper into 10 equally sized chunks and measure how many of these chunks contain any sentence from $S_{15}$ . The coverage rate is defined as:  

Figure 4: Pairwise similarities between pairs of 10 questions generated from Ivleva et al. (2009). Lower similarity is indicated by bluer cells, while higher similarity is indicated by redder cells. Scores above 0.7 are marked with red dots.  

$$
P_{\mathrm{cov\_rate}}={\frac{\sum_{i=1}^{10}\bigcup_{x\in t o p k(a)}\mathbb{1}_{x}(C_{i}))}{a\in A}}
$$  

where $C_{i}$ is the set of sentences in chunk $i$ of the paper $(i\in1,2,..,10)$ , $A$ is the set of generated answers, $t o p k(a)$ is the set of $15\%$ of sentences in the paper that are the most similar to answer $a$ , and $\mathbb{1}_{x}(A)$ is the indicator function indicating if $x\in A$ .  

We calculate a similarity score between each answer and every sentence in the paper using contextual embeddings generated by the e5-mistral-7b-instruct model (Wang et al., 2024). The average coverage rate for the 50 papers randomly selected is $78.6\%$ . Details of how sourcing works in practice are visualized in Appendix G.  

# 5 Experiments  

# 5.1 Experimental setup  

To investigate performance differences across various language models and the impact of fine-tuning, we designed a comprehensive experimental setup.  

Zero-shot setting. We used part of the test set in SciQAG-24D to conduct a zero-shot comparison of five models. Two of them are open-sourced LLMs: LLaMA1 (7B) (Touvron et al., 2023a) and LLaMA2-chat (7B) (Touvron et al., 2023b). And the rest are commercial LLM systems (use API): GPT3.5 (gpt-3.5-turbo) (Ouyang et al., 2022), GPT-4 (gpt-4-1106-preview) (OpenAI et al., 2023), and Claude3 (claude-3-opus-20240229) (Anthropic, 2024). Each model was prompted with 1000 questions in test, and their outputs were evaluated by CAR metrics (see Section 5.3 for details) to gauge their zero-shot capabilities in answering scientific research questions.  

Fine-tuned setting. Next, we examined the impact of fine-tuning. We fine-tuned LLaMA1 using the training set in SciQAG-24D. This fine-tuned model, LLaMA1-QA, was evaluated on an unseen test set in SciQAG-24D, and its performance was compared against the zero-shot performance of the original LLaMA1. Since the primary purpose of the training set is to fine-tune the model to infuse scientific knowledge, we further explored the performance of LLaMA1-QA after fine-tuning on other scientific tasks. The first task was SciQ (Welbl et al., 2017), a scientific MCQ benchmark and we used 10,000 examples as training data and 1,000 examples as test data. We also conducted evaluations on several other scientific tasks (see Table 5 for details), including different types like classification, regression, and transformation.  

# 5.2 Implementation details  

For zero-shot experiments, we set repetition penalty of LLaMA1 to 1.2 and max token length of LLaMA2- chat to 1024. Apart from these specific settings, the rest are configured with repetition set to 1 and the max token length set to 512. To fine-tune the LLaMA1 model on QA pairs, we follow the method of Alpaca (Taori et al., 2023), using a batch size of 1 and a maximum sequence length of 512 tokens. The training process is conducted over 2 epochs using the DeepSpeed Stage 3 optimization framework across 8 AMD GPUs. Gradients are accumulated over 2 steps, and the learning rate is set to 2e-5 with no weight decay. The number of warm-up steps is set to 20. For all inference, we set the temperature to 0.8 and the top_p to 0.75.  

# 5.3 Evaluation metrics  

For the evaluation of the quality of generated answers in the zero-shot experiment, we adopt a modified version of the RACAR framework. We exclude the criteria of "Relevance" and "Agnosticism", which are specifically designed to assess the alignment of question-answer (QA) pairs with a given paper, and name this modified metric as CAR (see Appendix J). For evaluating performance on SciQ dataset, we utilize the accuracy metric to conduct our evaluations. For evaluating performance on other scientific tasks, we use F1-score for classification tasks, Mean Absolute Error (MAE) for regression tasks, and KL divergence (Brown et al., 2019) for transformation tasks.  

# 6 Results and Analysis  

# 6.1 Zero-shot Evaluation  

The results of the zero-shot performance on the SciQAG-24D test are summarized in Table 2. Among the models, GPT-4 achieved the highest scores in Completeness (4.9) and Reasonableness (4.9896), while Claude3 obtained the highest Accuracy score (4.95). GPT-3.5 also performed exceptionally well, with scores closely trailing those of GPT-4 and Claude3 across all metrics. Notably, LLaMA1 scores the lowest in all three dimensions. In comparison, the LLaMA2-chat model, although not as high-scoring as the GPT models, demonstrates substantial improvements over the original LLaMA1 in all metrics. These results highlight the superior zero-shot performance of the commercial LLM models in answering science questions. We can also find that the latest open-source models, such as LLaMA2-chat, have made significant advancements and possess science questionanswering capabilities comparable to commercial LLMs.  

# 6.2 Impact of Fine-tuning  

We selected LLaMA1, which exhibited the poorest zero-shot performance, for fine-tuning to obtain LLaMA1-QA. Through three experiments, we demonstrated that SciQAG-24D serves as effective fine-tuning data capable of improving downstream scientific task performance.  

SciQAG-24D test. As shown in the last line of Table 2, LLaMA1-QA, demonstrated significant performance improvement compared to the original  

<html><body><table><tr><td>Model</td><td>Completeness</td><td>Accuracy</td><td>Reasonableness</td></tr><tr><td>LLaMA1</td><td>2.7813</td><td>2.9625</td><td>3.2917</td></tr><tr><td>LLaMA2-chat</td><td>4.7125</td><td>4.4708</td><td>4.7625</td></tr><tr><td>GPT-3.5</td><td>4.8708</td><td>4.9333</td><td>4.9895</td></tr><tr><td>GPT-4</td><td>4.9</td><td>4.9438</td><td>4.9896</td></tr><tr><td>Claude3</td><td>4.7938</td><td>4.95</td><td>4.9875</td></tr><tr><td>LLAMA1-QA</td><td>3.1479 (+13.18%)</td><td>3.8875 (+31.22%)</td><td>4.3896 (+33.35%)</td></tr></table></body></html>

Table 2: Zero-shot performance on SciQAG-24D test. Bold denotes best model scores. As a comparison with LLaMA1, we also show results of LLaMA1_QA in the last line.  

Table 3: Fine-tuning performance of LLaMA1 and LLAMA1-QA on SciQ and scientific tasks (C for classification; R for regression; T for transformation). The details of datasets are available in Table 5.   


<html><body><table><tr><td>Type</td><td>Dataset</td><td>LLaMA1 LLAMA1-QA</td></tr><tr><td>M</td><td>SciQ Glass</td><td>0.895 0.904</td></tr><tr><td>C</td><td>Pei Stability UV</td><td>0.813 0.872 0.9531 0.960 0.809 0.931 0.596 0.729</td></tr><tr><td>R</td><td>Bandgap Steels ChEMBL Diversity</td><td>0.435 0.384 463.959 185.316 1.195 1.180 0.307 0.294</td></tr><tr><td>T</td><td>Cp Stability ESOL</td><td>4.790 4.645 0.647 0.824 0.788 0.829</td></tr></table></body></html>  

LLaMA1 (over $13\%$ in Completeness, and over $30\%$ in Accuracy and Reasonableness). This indicates that LLaMA1 has learned the logic of answering science questions from the training data in SciQAG24D and internalized some scientific knowledge.  

SciQ test. In the first line of Table 3, LLaMA1-QA exhibits a slight improvement over LLaMA1 $(+1\%)$ . However, based on our observations and statistics, fine-tuning on SciQAG-24D has also enhanced the model’s instruction-following ability: the SciQ test requires selecting an option as an answer, but occasionally, LLMs may generate format errors that prevent automatic parsing of the predicted option from the output. The probability of unparseable outputs has decreased from $4.1\%$ with LLaMA1 to $1.7\%$ with LLaMA1-QA.  

Scientific tasks. In Table 3, LLaMA1-QA exhibits notable improvements over LLaMA1 model in scientific tasks. The most significant improvement is observed in the Steels regression task, where the MAE decreased from 463.96 to 185.32. These findings indicate that incorporating science QA into the training process can enhance the model’s ability to learn scientific knowledge, thereby improving its performance on downstream prediction tasks. Surprisingly, compared to machine learning models specifically designed, LLM can achieve comparable results or even surpass them in certain tasks. For example, in the Bandgap task, although LLaMA1- QA does not perform as well as models like MODNet (0.3327), it has already surpassed AMMExpress v2020 (0.4161) (Dunn et al., 2020). In the Diversity task, LLaMA1-QA outperforms the deep learning baseline (0.3198) (Jablonka et al., 2024). These findings demonstrates that LLMs hold significant potential for specific scientific tasks.  

# 7 Conclusion  

In this work, we present SciQAG, a novel framework for generating QA pairs from scientific literature, and propose RACAR metrics for evaluating and filtering these QA pairs. SciQAG can cost-effectively produce knowledge-based QA data for low-resource scientific domains, addressing a key challenge in developing science QA systems. Using SciQAG, we create an science QA dataset with 188,042 QA pairs. Its training set can finetune LLMs with scientific knowledge, and its test set can evaluate LLM performance on open-ended, closed-book science QA tasks. To assess the impact of the dataset, we compare the zero-shot performance of several LLMs on the test set and finetune LLaMA1 on the training set, resulting in the LLaMA1-QA model. This fine-tuning significantly improves LLaMA1-QA’s performance on the test set (unseen QA), the SciQ test (MCQ), and various scientific tasks (real-world experimental problems). Additionally, this study demonstrates the potential of LLMs in scientific tasks, with LLaMA1-QA achieving results comparable to or surpassing machine learning baselines. These findings highlight the utility of the SciQAG-24D dataset in enhancing LLMs’ understanding and application of scientific knowledge. We aim to promote future research in this field, advancing science QA systems and providing valuable resources for practitioners.  

# Limitations  

Firstly, although the questions in the dataset do not explicitly depend on the original text, there may still be implicit dependencies, such as authorspecific terminology or names. This can lead to gap between what the LLM learns during training and real knowledge embedded in the QA pairs. Secondly, hallucinations, while relatively infrequent, are still inevitable due to the inherent nature of LLMs. Even a perfect RACAR score does not guarantee that the generated answer is entirely accurate or factual. Thirdly, the RACAR evaluation framework appears to perform better when using commercial models compared to open-source models, and when assessing each dimension individually rather than all five dimensions simultaneously. However, this approach is more resourceintensive. Future work should explore the balance between computational efficiency and performance to develop more sustainable evaluation strategies. Lastly, the current version of the SciQAG-24D dataset covers a limited range of scientific domains. To enhance the dataset’s representativeness and utility, it is crucial for the research community to collaborate and contribute to its expansion, incorporating a wider variety of scientific disciplines and question types.  

# Ethics Statement  

Our research does not raise any major ethical concerns.  

sciqa scientific question answering benchmark for scholarly knowledge. Scientific Reports, 13(1):7240.  

Satanjeev Banerjee and Alon Lavie. 2005a. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, Michigan. Association for Computational Linguistics.  

Satanjeev Banerjee and Alon Lavie. 2005b. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65–72.  

Rohit Batra, Carmen Chen, Tania G Evans, Krista S Walton, and Rampi Ramprasad. 2020. Prediction of water stability of metal–organic frameworks using machine learning. Nature Machine Intelligence, 2(11):704–710.  

Edward J. Beard, Ganesh Sivaraman, Álvaro VázquezMayagoitia, Venkatram Vishwanath, and Jacqueline M. Cole. 2019. Comparative dataset of experimental and computational attributes of UV/vis absorption spectra. Scientific Data, 6(1):307. Publisher: Nature Publishing Group.  

Nathan Brown, Marco Fiscato, Marwin HS Segler, and Alain C Vaucher. 2019. Guacamol: benchmarking models for de novo molecular design. Journal of chemical information and modeling, 59(3):1096– 1108.  

Anthony Chen, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Evaluating question answering evaluation. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 119–124, Hong Kong, China. Association for Computational Linguistics.  

Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer opendomain questions. arXiv preprint arXiv:1704.00051.  

Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023. LongLoRA: Efficient fine-tuning of long-context large language models.  

# References  

Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Retrieved from https://www-cdn.anthropic.com/ de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/ Model_Card_Claude_3.pdf.  

Sören Auer, Dante AC Barone, Cassiano Bartz, Eduardo G Cortes, Mohamad Yaser Jaradeh, Oliver Karras, Manolis Koubarakis, Dmitry Mouromtsev, Dmitrii Pliukhin, Daniil Radyush, et al. 2023. The  

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny  

Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling language modeling with pathways.  

Elizabeth Clark, Asli Celikyilmaz, and Noah A. Smith. 2019. Sentence mover’s similarity: Automatic evaluation for multi-sentence texts. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2748–2760, Florence, Italy. Association for Computational Linguistics.  

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.  

Tri Dao. 2023. FlashAttention-2: Faster attention with better parallelism and work partitioning.  

Bidyut Das, Mukta Majumder, Santanu Phadikar, and Arif Ahmed Sekh. 2021. Automatic question generation and answer assessment: A survey. Research and Practice in Technology Enhanced Learning, 16(1):1– 15.  

Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599–4610, Online. Association for Computational Linguistics.  

John S Delaney. 2004. ESOL: estimating aqueous solubility directly from molecular structure. Journal of chemical information and computer sciences, 44(3):1000–1005.  

Alexander Dunn, Qi Wang, Alex Ganose, Daniel Dopp, and Anubhav Jain. 2020. Benchmarking materials property prediction methods: the Matbench test set and Automatminer reference algorithm. npj Computational Materials, 6(1):138.  

Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and Huajun Chen. 2024. Mol-Instructions – A large-scale biomolecular instruction dataset for large language models. In 12th International Conference on Learning Representations.  

Anna Gaulton, Louisa J Bellis, A Patricia Bento, Jon Chambers, Mark Davies, Anne Hersey, Yvonne Light, Shaun McGlinchey, David Michalovich, and Bissan Al-Lazikani. 2012. ChEMBL: a large-scale bioactivity database for drug discovery. Nucleic acids research, 40(D1):D1100–D1107.  

Michael Heilman and Noah A Smith. 2010. Good question! statistical ranking for question generation. In Human language technologies: The 2010 annual conference of the North American Chapter of the Association for Computational Linguistics, pages 609–617.  

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.  

Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. Advances in neural information processing systems, 28.  

Natalia P Ivleva, Michael Wagner, Harald Horn, Reinhard Niessner, and Christoph Haisch. 2009. Towards a nondestructive chemical characterization of biofilm matrix by raman microscopy. Analytical and bioanalytical chemistry, 393:197–206.  

Kevin Maik Jablonka, Philippe Schwaller, Andres Ortega-Guerrero, and Berend Smit. 2024. Leveraging large language models for predictive chemistry. Nature Machine Intelligence, pages 1–9.  

Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024. Mixtral of experts.  

Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What disease does this patient have? A large-scale open domain question answering dataset from medical exams. $A p$ - plied Sciences, 11(14).  

Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: A dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2567– 2577, Hong Kong, China. Association for Computational Linguistics.  

Ehsan Kamalloo, Nouha Dziri, Charles Clarke, and Davood Rafiei. 2023. Evaluating open-domain question answering in the era of large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5591–5606, Toronto, Canada. Association for Computational Linguistics.  

Yoshiyuki Kawazoe, J-Z Yu, A-P Tsai, and T Masumoto. 1997. Nonequilibrium phase diagrams of ternary amorphous alloys.  

Rudolf Krska and Alexandra Molinelli. 2009. Rapid test strips for analysis of mycotoxins in food and feed. Analytical and bioanalytical chemistry, 393:67–71.  

Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234–1240.  

Seongyun Lee, Hyunjae Kim, and Jaewoo Kang. 2023a. LIQUID: A framework for list question answering dataset generation. arXiv preprint arXiv:2302.01691.  

Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-in Lee, and Moontae Lee. 2023b. QASA: Advanced question answering on scientific articles. In Proceedings of the 40th International Conference on Machine Learning.  

Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.  

Shayne Longpre, Yi Lu, and Joachim Daiber. 2021. Mkqa: A linguistically diverse benchmark for multilingual open domain question answering. Transactions of the Association for Computational Linguistics, 9:1389–1406.  

Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. In Advances in Neural Information Processing Systems.  

Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. Chartqa: A benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244.  

Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? A new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381–2391, Brussels, Belgium. Association for Computational Linguistics.  

Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12076–12100, Singapore. Association for Computational Linguistics.  

Ruslan Mitkov et al. 2003. Computer-aided generation of multiple-choice tests. In Proceedings of the HLTNAACL 03 workshop on Building educational applications using natural language processing, pages 17–22.  

Seyed Mohamad Moosavi, Aditya Nandy, Kevin Maik Jablonka, Daniele Ongari, Jon Paul Janet, Peter G Boyd, Yongjin Lee, Berend Smit, and Heather J Kulik. 2020. Understanding the diversity of the metalorganic framework ecosystem. Nature communications, 11(1):1–10.  

Nikahat Mulla and Prachi Gharpure. 2023. Automatic question generation: A review of methodologies, datasets, evaluation metrics, and applications. Progress in Artificial Intelligence, 12(1):1–32.  

penAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,  

Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2023. GPT-4 technical report.  

Arantxa Otegi, Iñaki San Vicente, Xabier Saralegi, Anselmo Peñas, Borja Lozano, and Eneko Agirre. 2022. Information retrieval and question answering: A case study on COVID-19 scientific literature. Knowledge-Based Systems, 240:108072.  

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, and Alex Ray. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744.  

Hariom A. Pandya and Brijesh S. Bhatt. 2021. Question answering survey: Directions, challenges, datasets, evaluation matrices.  

Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.  

Zongrui Pei, Junqi Yin, Jeffrey A Hawk, David E Alman, and Michael C Gao. 2020. Machine-learning informed prediction of high-entropy solid solution formation: Beyond the Hume-Rothery rules. npj Computational Materials, 6(1):50.  

Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. 2021. MAUVE: Measuring the gap between neural text and human text using divergence frontiers. In Advances in Neural Information Processing Systems.  

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: $100{,}000{+}$ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin, Texas. Association for Computational Linguistics.  

Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. DeepSpeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505–3506.  

Zafaryab Rasool, Stefanus Kurniawan, Sherwin Balugo, Scott Barnett, Rajesh Vasa, Courtney Chesser, Benjamin M. Hampstead, Sylvie Belleville, Kon Mouzakis, and Alex Bahar-Fuchs. 2024. Evaluating LLMs on document-based QA: Exact answer selection and numerical extraction using cogtale dataset.  

Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? arXiv preprint arXiv:2002.08910.  

Gaurav Sahu, Pau Rodriguez, Issam Laradji, Parmida Atighehchian, David Vazquez, and Dzmitry Bahdanau. 2022. Data augmentation for intent classification with off-the-shelf large language models. In Proceedings of the 4th Workshop on NLP for Conversational AI, pages 47–57, Dublin, Ireland. Association for Computational Linguistics.  

Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. 2022. ScienceQA: A novel resource for question answering on scholarly articles. International Journal on Digital Libraries, 23(3):289–301.  

Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022. A-okvqa: A benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, pages 146–162. Springer.  

Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2022. Large language models encode clinical knowledge. arXiv preprint arXiv:2212.13138.  

Yu Song, Santiago Miret, Huan Zhang, and Bang Liu. 2023. HoneyBee: Progressive instruction finetuning of large language models for materials science. In  

Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5724–5739, Singapore. Association for Computational Linguistics.  

Anand Subramanian, Viktor Schlegel, Abhinav Ramesh Kashyap, Thanh-Tung Nguyen, Vijay Prakash Dwivedi, and Stefan Winkler. 2024. M-qalm: A benchmark to assess clinical reading comprehension and knowledge recall in large language models via question answering.  

Tianyi Tang, Hongyuan Lu, Yuchen Eleanor Jiang, Haoyang Huang, Dongdong Zhang, Wayne Xin Zhao, and Furu Wei. 2023. Not all metrics are guilty: Improving NLG evaluation with LLM paraphrasing.  

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.  

Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085.  

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.  

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models.  

Altaj Virani, Rakesh Yadav, Prachi Sonawane, and Smita Jawale. 2023. Automatic question answer generation using T5 and NLP. In International Conference on Sustainable Computing and Smart Systems, pages 1667–1673.  

Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Improving text embeddings with large language models.  

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-Instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560.  

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-Instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484–13508, Toronto, Canada. Association for Computational Linguistics.  

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824–24837. Curran Associates, Inc.  

Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy Usergenerated Text, pages 94–106, Copenhagen, Denmark. Association for Computational Linguistics.  

Tong Xie, Yuwei Wan, Wei Huang, Zhenyu Yin, Yixuan Liu, Shaozhou Wang, Qingyuan Linghu, Chunyu Kit, Clara Grazian, Wenjie Zhang, Imran Razzak, and Bram Hoex. 2023a. DARWIN series: Domain specific large language models for natural science.  

Tong Xie, Yuwei Wan, Wei Huang, Yufei Zhou, Yixuan Liu, Qingyuan Linghu, Shaozhou Wang, Chunyu Kit, Clara Grazian, Wenjie Zhang, et al. 2023b. Large language models as master key: unlocking the secrets of materials science with gpt. arXiv preprint arXiv:2304.02213.  

Tong Xie, Yuwei Wan, Ke Lu, Wenjie Zhang, Chunyu Kit, and Bram Hoex. 2023c. Tokenizer effect on functional material prediction: Investigating contextual word embeddings for knowledge discovery. In AI for Accelerated Materials Design-NeurIPS 2023 Workshop.  

Tong Xie, Yuwei Wan, Yufei Zhou, Wei Huang, Yixuan Liu, Qingyuan Linghu, Shaozhou Wang, Chunyu Kit, Clara Grazian, Wenjie Zhang, et al. 2024. Creation of a structured solar cell material dataset and performance prediction using large language models. Patterns, 5(5).  

Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2024. Hallucination is inevitable: An innate limitation of large language models. arXiv preprint arXiv:2401.11817.  

Yanpeng Ye, Jie Ren, Shaozhou Wang, Yuwei Wan, Imran Razzak, Tong Xie, and Wenjie Zhang. 2024. Construction of functional materials knowledge graph in multidisciplinary materials science via large language model. arXiv preprint arXiv:2404.03080.  

Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, and Woomyoung Park. 2021. GPT3Mix: Leveraging large-scale language models for text augmentation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2225–2239, Punta Cana, Dominican Republic. Association for Computational Linguistics.   
Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, et al. 2024a. Chemllm: A chemical large language model. arXiv preprint arXiv:2402.06852.   
Qiang Zhang, Keyang Ding, Tianwen Lyv, Xinda Wang, Qingyu Yin, Yiwen Zhang, Jing Yu, Yuhao Wang, Xiaotong Li, Zhuoyi Xiang, et al. 2024b. Scientific large language models: A survey on biological & chemical domains. arXiv preprint arXiv:2401.14656.   
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating text generation with BERT. In International Conference on Learning Representations.   
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-bench and Chatbot Arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track.   
Ya Zhuo, Aria Mansouri Tehrani, and Jakoah Brgoch. 2018. Predicting the band gaps of inorganic solids by machine learning. The journal of physical chemistry letters, 9(7):1668–1673.  

# A Complete list of 24 WoS categories and original distribution  

1. Materials Science, Multidisciplinary   
2. Physics, Multidisciplinary   
3. Chemistry, Multidisciplinary   
4. Materials Science, Coatings & Films   
5. Materials Science, Ceramics   
6. Materials Science, Characterization & Testing   
7. Materials Science, Biomaterials   
8. Materials Science, Composites   
9. Materials Science, Paper & Wood   
10. Materials Science, Textiles   
1. Physics, Applied   
2. Physics, Atomic, Molecular & Chemical   
3. Physics, Condensed Matter   
14. Physics, Fluids & Plasmas   
15. Physics, Mathematical   
16. Physics, Nuclear   
17. Physics, Particles & Fields   
18. Chemistry, Analytical   
19. Chemistry, Applied   
20. Chemistry, Inorganic & Nuclear   
21. Chemistry, Medicinal   
22. Chemistry, Organic   
23. Chemistry, Physical   
24. Energy & Fuels  

# B Statistics of SciQAG-24D dataset  

Table 4 shows the details of 24 categories in SciQAG-24D dataset.  

# C Prompts for QA generator  

Here is a scientific paper:   
{text}   
Here is a two-step task for you.   
Step 1: Read given scientific paper and extract a list of 15 keywords focusing on the important terms and concepts within the paragraph. Avoid generic or broad words.   
Step 2: Generate 10 scientific Q&A pairs as diverse as possible based on facts and knowledge presented in given paper, focusing on keywords you generated. Keep the following requirements in mind: Avoid ask simple or definitional questions. Please assume that there is no corresponding paper to refer to when asking questions, so ensure that the questions and answers are self-contained (do not refer to external sources like figures or tables, or use demonstrative pronouns such as "this"). Incorporate specific data and insights from the paper to create detailed and informative answers. Ensure that the answers are concise, accurate, and directly related to the corresponding questions. Please present the generated keywords and question-answer pairs in the following format: Keywords: [keyword 1], [keyword 2], ..., [keyword15]   
Q1: [Question 1]   
A1: [Answer 1]  

Q2: [Question 2]  

Figure 5: Original distribution of papers from the WoS Core Collection across $24\mathrm{WoS}$ categories selected from Chemistry, Physics, Materials Science and Energy.   
Table 4: Statistics of SciQAG-24D dataset.   


<html><body><table><tr><td rowspan="2">Category</td><td>Original</td><td colspan="4">Support (train)</td><td colspan="2">Support (test)</td></tr><tr><td>Paper</td><td>Paper</td><td>QA</td><td>Paper_filter</td><td>QA_filter</td><td>Paper_filter</td><td>QA_filter</td></tr><tr><td>Chemistry, Analytical</td><td>1020</td><td>970</td><td>9700</td><td>970</td><td>8075</td><td>50</td><td>375</td></tr><tr><td>Chemistry, Applied</td><td>1037</td><td>987</td><td>9870</td><td>987</td><td>8314</td><td>50</td><td>385</td></tr><tr><td>Physics, Condensed Matter</td><td>1025</td><td>975</td><td>9750</td><td>974</td><td>8340</td><td>50</td><td>366</td></tr><tr><td>Chemistry, Organic</td><td>1048</td><td>998</td><td>9980</td><td>998</td><td>8513</td><td>50</td><td>369</td></tr><tr><td>Materials Science, Multidisciplinary</td><td>1000</td><td>950</td><td>9500</td><td>950</td><td>8382</td><td>50</td><td>378</td></tr><tr><td>Chemistry, Inorganic & Nuclear</td><td>1024</td><td>974</td><td>9740</td><td>973</td><td>8225</td><td>50</td><td>333</td></tr><tr><td>Physics, Nuclear</td><td>941</td><td>891</td><td>8910</td><td>888</td><td>7208</td><td>50</td><td>374</td></tr><tr><td>Physics, Atomic, Molecular & Chemical</td><td>1081</td><td>1031</td><td>10310</td><td>1031</td><td>8215</td><td>50</td><td>324</td></tr><tr><td>Materials Science,Paper & Wood</td><td>375</td><td>325</td><td>3250</td><td>325</td><td>2743</td><td>50</td><td>319</td></tr><tr><td>Physics, Mathematical</td><td>1036</td><td>986</td><td>9860</td><td>985</td><td>7126</td><td>50</td><td>257</td></tr><tr><td>Physics, Particles & Fields</td><td>1000</td><td>950</td><td>9500</td><td>944</td><td>7361</td><td>50</td><td>341</td></tr><tr><td>Materials Science,Biomaterials</td><td>1007</td><td>957</td><td>9570</td><td>957</td><td>8093</td><td>50</td><td>357</td></tr><tr><td> Physics, Multidisciplinary</td><td>1000</td><td>950</td><td>9500</td><td>950</td><td>8009</td><td>50</td><td>358</td></tr><tr><td>Physics, Applied</td><td>581</td><td>531</td><td>5310</td><td>531</td><td>4699</td><td>50</td><td>391</td></tr><tr><td>Energy & Fuels</td><td>1076</td><td>1026</td><td>10260</td><td>1026</td><td>8625</td><td>50</td><td>390</td></tr><tr><td>Materials Science, Coatings & Films</td><td>1004</td><td>954</td><td>9540</td><td>954</td><td>8390</td><td>50</td><td>377</td></tr><tr><td>Chemistry, Multidisciplinary</td><td>952</td><td>902</td><td>9020</td><td>901</td><td>7777</td><td>50</td><td>368</td></tr><tr><td>Chemistry, Medicinal</td><td>570</td><td>520</td><td>5200</td><td>520</td><td>4302</td><td>50</td><td>335</td></tr><tr><td>Materials Science, Textiles</td><td>1023</td><td>973</td><td>9730</td><td>973</td><td>8095</td><td>50</td><td>349</td></tr><tr><td>Materials Science, Characterization & Testing</td><td>1048</td><td>998</td><td>9980</td><td>998</td><td>8517</td><td>49</td><td>367</td></tr><tr><td>Physics, Fluids & Plasmas</td><td>1016</td><td>966</td><td>9660</td><td>965</td><td>7085</td><td>50</td><td>326</td></tr><tr><td>Materials Science, Composites</td><td>1046</td><td>996</td><td>9960</td><td>996</td><td>8541</td><td>50</td><td>376</td></tr><tr><td>Chemistry, Physical</td><td>791</td><td>741</td><td>7410</td><td>741</td><td>6380</td><td>50</td><td>358</td></tr><tr><td>Materials Science, Ceramics</td><td>1042</td><td>992</td><td>9920</td><td>992</td><td>8496</td><td>50</td><td>358</td></tr><tr><td>Sum</td><td>22743</td><td>21543</td><td>215430</td><td>21529</td><td>179511</td><td>1199</td><td>8531</td></tr></table></body></html>  

A2: [Answer 2]  

···  

# D GPT-4 prompts for RACAR metric  

Relevance: Given a scientific paper and questions generated from it, evaluate the relevance of the question to the paper and return a score ranging from 1–3 and give reasons as to why this score was assigned. The output must be a list of dictionaries corresponding to each question, with the fields ‘score’ and ‘reasons.’ If the question does not pertain to the paper, assign a score of 1.  

Paper: { paper_text }  

Questions: { [ { questions } ] }  

Output:  

Agnosticism: Given questions generated from a scientific article, evaluate its context independence and return a score ranging from 1–3. Identify whether the question is referring to specific experimental setups, figures, or tables from the paper. For example, questions like “What can we say about the effect of varying pressure from Figure 1?” should be assigned a score of 1. The output must be a list of dictionaries corresponding to each question, with the fields ‘score’ and ‘reasons.’  

Questions: { [ { questions } ] }  

# Output:  

Completeness: Given a scientific paper and question answer pairs generated from it, evaluate the completeness of the answer for each question and return a score ranging from 1–3 indicating the extent to which the answer fully addresses the question using the information in the paper, including all subquestions. Also give reasons for assigning the score. The output must be a list of dictionaries for each question answer pair, with the fields ‘score’ and ‘reasons.’  

Paper: { paper_text }  

Questions: { [ { qa_pairs } ] }  

Output:  

Accuracy: Given a scientific paper and question answer pairs generated from this scientific paper, evaluate the accuracy of the answer for each question and return a score ranging from 1–3 indicating whether the answer is accurately extracted from the paper and give reasons as to why this score was assigned. This involves checking the accuracy of any claims or statements made in the text, and verifying that they are supported by evidence.  

The output must be a list of dictionaries for each question answer pair, with the fields ‘score’ and ‘reasons.’  

Paper: { paper_text }  

QA pairs: { [ {qa_pairs} ] }  

Output:  

Reasonableness: Given a scientific paper and statements, evaluate the reasonableness of the statements with respect to the paper and return a score ranging from 1–3 indicating how logically consistent the content is, with no obvious contradictions and provide reasons for assigning the score. The output must be a list of dictionaries for each statement, with the fields ‘score’ and ‘reasons.’ Assign a score of 1 if the statement has logical error like contradicts.  

Paper: { paper_text }  

Statements: { [ { answers } ] }  

Output:  

# E Prompts for similarity of questions  

""" $\mathrm{\Phi}_{<s>}$ [INST] \*\*Objective\*\*: Assess the similarity of two questions not just based on their word content and relatedness, but primarily based on their underlying purpose or intent. This assessment should distinguish between questions that are about opposite aspects (even if related) and those that are different in wording but similar in the information they seek. Mark it from 0–100. 0 means absolutely no same intention, 100 means completely same intention. You don’t need to explain your choice, just output a score in the format of similarity score: <the score>.  

\*\*Procedure\*\*:  

1. Read and Understand the Questions:   
- Carefully read the given questions.   
- Understand the core purpose behind each question.   
2. Identify Key Aspects:   
- Determine what each question is specifically asking for.   
- Identify if the questions are seeking positive, negative, or neutral aspects of a subject.   
3. Analyze Semantic Differences:   
- Compare the intent behind the questions.   
- Look for opposing themes such as benefits vs drawbacks, advantages vs disadvantages, etc. 4. Similarity Scoring: - Assign a high similarity score if both questions are asking about the same aspect of a subject, even if the wording is different.   
- Assign a low similarity score if the questions are asking about opposite aspects, even if they are related or use similar words.  

\*\*Examples\*\*:  

Question 1: “What are the pros of using zeolites?” Question 2: “What are the drawbacks of using zeolites?”   
Analysis: Although both questions are about zeolites, they asks different aspects: positive vs. negative.  

Similarity Score: 0  

Question 1: “Give some examples of using zeolites as catalysts."   
Question 2: “What are potential applications of using zeolites as catalysts?”   
Analysis: Both questions seek information on how zeolites are used as catalysts, with slightly different wording.  

Similarity Score: 100 \*\*Notes:\*\*  

Pay attention to nuanced language that may indicate different intentions even in closely related topics.  

Remember, a high degree of relatedness does not necessarily imply a high degree of similarity in intent.  

\*\*Questions\*\*   
Question1: sent1   
Question2: sent2 [/INST]"""  

# F Self-questioning vs. SciQAG  

Self-questioning (Fang et al., 2024)  

# Prompt:  

Given a paragraph of paper, can you ask five questions and give answers that can be directed to the paragraph? List in the format of Question-Answer pair.  

Paragraph: {paragraph}  

Example GPT-3.5 completion:  

Q1: What is being discussed in this paragraph?  

A1: This paragraph is discussing the synthesis of one-dimensional carbon nanorods through a self-templated, catalyst-free strategy and the potential application of these nanorods and graphene nanoribbons in supercapacitor electrodes.  

Q2: What transforms the nanorods into graphene nanoribbons?   
A2: Sonochemical treatment followed by chemical activation transforms the nanorods into graphene nanoribbons.  

Q3: What properties of nanomaterials makes them attractive? A3: The extraordinary electrical, mechanical and thermal properties of one- and two-dimensional carbon nanomaterials makes them attractive.  

Q4: What is the major challenge associated with the synthesis of these materials? A4: The major challenge in the synthesis of these materials is to develop simple and efficient methods that consume low amounts of energy.  

Q5: What advantages does this particular synthetic approach offer?  

A5: This particular synthetic approach offers advantages such as scalability and the production of functionally useful materials on industrial levels.  

SciQAG (this paper)  

# Prompt:  

Here is a scientific paper: {text}  

Given the provided scientific paper, please complete the following two steps:  

Step 1: Keyword Extraction Read the scientific paper and identify 15 keywords that capture the most important terms and concepts in the paper, avoiding generic or broad terms. Compile the selected keywords into a list.  

Step 2: Question-Answer Generation  

Generate 10 science question-answer pairs as diverse as possible based on facts and knowledge presented in the given paper, focusing on keywords you generated. Keep the following requirements in mind: Avoid asking simple or definitional questions. Assume that the reader does not have access to the original paper or any external sources, so ensure that the questions and answers are self-contained and do not rely on references to figures, tables, or other parts of the paper. Incorporate specific data and insights from the paper to provide detailed and informative answers. Keep the answers concise, accurate, and directly related to the corresponding questions.  

Please present the generated keywords and question-answer pairs in the following format: Keywords: [keyword 1], [keyword 2], ..., [keyword15]  

Q1: [Question 1] A1: [Answer 1]  

Q2: [Question 2] A2: [Answer 2]  

···  

Q10: [Question 10] A10: [Answer 10]  

# Example SciQAG completion:  

Q1: What is Type 1 diabetes and what causes it? A1: Type 1 diabetes is an autoimmune disease that results from the selective destruction of insulin-producing beta cells in the pancreatic islets. Genetics, environmental factors, nutritional effects, and a combination of these are believed to be associated with the disease. Streptozotocin (STZ) is commonly used to induce diabetes mellitus in experimental studies.  

Q2: What is the role of metabolomics in healthcare?  

A2: Metabolomics is a rapidly growing field that relates biological end points to multiple altered metabolite concentrations, providing a wealth of biological information on complex systems. It has been applied to a variety of diseases such as cancer, type 2 diabetes, and inborn errors of metabolism. It uses advanced analytical techniques such as nuclear magnetic resonance (NMR) spectroscopy and mass spectrometry with multivariate statistical analysis to identify potential biomarkers and biological networks.  

Q3: How was the diabetic rat model established in this study?  

A3: In this study, female rats were divided into a diabetic group and a control group. The diabetic group received an intraperitoneal dose of STZ, and showed glucose levels in the blood of more than $200~\mathrm{mg/dl}$ after 4 days. The control group was matched for age and gender, and did not receive the STZ injection.  

Q4: What were the key findings regarding glucose metabolism in the study?  

A4: The study found that in diabetes, glucose excretion was enormous, being nearly 7500-fold higher than in control rats. Other significantly enhanced metabolites included lactate, alanine, ethanol, and fumarate. These findings suggest that glucose metabolism is dramatically perturbed in diabetes.  

Q5: What is the importance of the Cori cycle in maintaining glucose homeostasis?  

A5: The Cori cycle, also known as the glucoselactate cycle, is a metabolic pathway that plays a crucial role in maintaining glucose homeostasis. It involves the conversion of glucose, lactate, and alanine, and is responsible for producing Cori cycle intermediates, which are used in gluconeogenesis precursor production and facilitating glucose oxidation.  

Q6: What is the role of acetate switch in metabolism?  

A6: The acetate switch, also known as the molecular switch that regulates the dissimilation and assimilation of acetate, plays a role in accumulating an acetyl–coenzyme A (CoA) pool due to both exogenous (bacterial fermentation) and endogenous (mammalian) production. This accumulation can have significant implications for metabolism.  

Q7: How did the urinary metabolites differ between diabetic and control rats in the study?  

A7: The urinary metabolites showed dramatic changes in the rat diabetic model when compared with blood-based assays. Glucose was the most notable metabolite in diabetic urine, with very high-intensity signals. Other metabolites such as alanine, lactate, ethanol, acetate, and fumarate also showed significant increases in the diabetic group.  

Q8: What was the effect of STZ administration on plasma glucose levels?  

A8: Following STZ administration, plasma glucose showed elevated levels, with its quantity being nearly three times higher than in control rats.  

Q9: What were some of the changes observed in the rat diabetic model’s blood parameters? A9: Other than the significant increases in blood glucose levels, the study also observed elevated levels of urea and creatinine, and decreased levels of plasma protein and cholesterol in diabetic rats.  

Q10: How did the correlations among the detected metabolites differ between the diabetic and control rats?  

A10: The correlations among the detected metabolites showed a number of striking differences between the diabetic and control rats. For example, the correlation of glucose with alanine was increased in diabetic rats, while the correlation of glucose with lactate decreased. These changes indicate significant alterations in metabolism in the diabetic rats.  

# Analysis:  

Due to the nature of its prompting technique, self-questioning is prone to generate more generic and contextually tied questions (e.g., “What is being discussed in this paragraph?”). SciQAG is able to avoid self-referential expressions such as ‘this,’ ‘that,’ and ‘these,’ making it more suitable for crafting challenging closed-book QA datasets, though this may slightly compromise its compatibility with source papers for open-book QA applications.  

The scope of questions generated by SciQAG is notably broad and deep. Unlike simpler, definitionfocused queries, SciQAG explores various aspects such as relevant properties, application impacts, strategic influences, structural differences, challenges, and experimental insights. This diversity ensures that the questions not only span a wide range of topics but also deeply engage with the subject matter of the scientific papers. The answers produced by SciQAG are equally impressive in their informativeness and specificity. They meticulously avoid over-generalization, often incorporating precise details like specific chemical elements and numerical values that are referenced in the original papers. This level of detail in the answers highlights the method’s ability to maintain relevance and accuracy in relation to the scientific content. The comparison between SciQAG and selfquestioning thus underscores SciQAG’s strengths in generating diverse, context-independent, and content-rich QA pairs.  

# G 10 generated questions example  

The following 10 questions were generated from a random paper Ivleva et al. (2009) by QA generator.  

Q1: What are the applications of rapid diagnostic assays as discussed in the paper?  

Q2: What is the significance of mycotoxin detection in the food and feed sector?  

Q3: What are the advantages of lateral-flow devices (LFDs) in the context of rapid immunoassay-based tests?  

Q4: What are the potential applications of LFDs according to the paper?  

Q5: What are the challenges in the development of rapid test strips for mycotoxins?  

Q6: What is the significance of antibody sensitivity and specificity in the performance of a membrane-based immunoassay?  

Q7: What extraction procedures are commonly used for mycotoxin analysis?  

Q8: How does matrix variation affect the results of rapid tests for mycotoxins?  

Q9: What is the role of colloidal gold in test strips for mycotoxins?  

Q10: How do regulatory limits vary for mycotoxins in different countries?  

# H Pairwise visualization  

We show in Figure 6 (a) and (b) the top 10 sentences with the highest similarity scores for each answer generated for two articles selected at random from our collection. We observe that some sentences serve as common sources for multiple answers; however, overall, there tend to be distinct sources for each answer. Additionally, we plot heatmaps to provide a holistic view of the similarity scores between each answer and all sentences in the paper. Figure 6 (c) and (d) offer a thorough overview of the connections between answers and various parts of the given paper (a & c, b & d are from the same paper). From the perspective both of coverage and sourcing rates, and of the two visualization methods, it is evident that the answers generated by SciQAG exhibit extensive coverage of the source paper and originate from diverse sources.  

# I Details of generating SciQAG-24D dataset  

We employ the gpt-4-1106-preview model with its default parameter settings for the seed QA generation and RACAR evaluation. We follow the training method from LongLoRA (Chen et al., 2023) and fine-tune the vicuna-7b-v1.5-16k model (Zheng et al., 2023) using Flash-Attention2 (Dao, 2023) and DeepSpeed (Rasley et al., 2020) stage 3 on an $8\times\mathsf{A800}$ GPU machine for 5 epochs with a per-device batch size of 1 and max length of 16,000 for both training and evaluation. Gradients were accumulated over 2 steps and the learning rate was set to 2e-5 with no weight decay. The warm-up steps were set to 20. For inference, we set temperature to 0.8 and top_p to 0.75, which makes text generated more logical with rich vocabulary.  

# J Prompts for CAR metric  

For this task, you are provided with a questionanswer pair. Evaluate the quality of answer on the following three criteria and record your evaluations in a score ranging from 1 to 5 for each criterion and provide reasons for assigning the score:  

1. \*\*Accuracy\*\*: Score the accuracy of the answer in relation to the question. A score of 5 means the answer is fully accurate. This involves checking the accuracy of any claims or statements made in the text, and verifying that they are supported by evidence. While a score of 1 indicates significant inaccuracies.  

2. \*\*Completeness\*\*: Rate how comprehensive the answer is. A score of 5 indicates that the answer addresses all key points of the question and includes sufficient background and supporting details and evidence. A score of 1 means the answer is largely incomplete.  

3. \*\*Reasonableness\*\*: Evaluate the logical consistency and reasonableness of the answer. A score of 5 indicates that the answer is logically sound with no contradictions; a score of 1 indicates that the answer contains major contradictions.  

Provide the scores in a dictionary output. The dictionary is with three keys (name of 3 criterions). The value is a tuple (score and comment).  

\*\*Example JSON Output:\*\*   
“‘json   
{{"Accuracy": (score, "reason"), "Completeness":   
(score, "reason"), "Reasonableness": (score,   
"reason")}}   
666   
\*\*Input Sections:\*\*   
- \*\*Question & Answer Pair\*\*:   
{qa}   
\*\*Output:\*\*  

# K Summary of datasets in scientific tasks  

Figure 6: Pairwise similarity from Krska and Molinelli (2009) and Ivleva et al. (2009), respectively. (a) and (b): The 10 sentences with the highest similarity scores. (c) and (d): Heatmap of all similarity scores.  

Table 5: Details of datasets and associated tasks they are used to derive instructions, (C for classification; R for regression; T for transformation)   


<html><body><table><tr><td>Datasets</td><td>Description</td><td>Task</td></tr><tr><td>Glass (Kawazoe et al.,1997)</td><td>This dataset is retrieved from a volume of the Lan- dolt- Bornstein collection 'Nonequilibrium phase diagrams of ternary amorphous alloys', containing data on full bulk metallic glass formation ability for</td><td>C</td></tr><tr><td>Pei (Pei et al.，2020)</td><td>5680 chemical formulas. The dataset is from a thermoelectric material review paper published in 2013, which abstracts information from over 100 publications. We used data of 1028 materials from this database.</td><td>C</td></tr><tr><td>Stability (Batra et al.，2020)</td><td>Consists of water stabilities for over 200 MOFs (metal-organic frameworks), alongside a comprehen- sive set of chemical features encompassing the metal node, organic ligand, and metal-ligand molar ratios.</td><td>C&T</td></tr><tr><td>Uv (Beard et al.， 2019)</td><td>Auto-generated UV/vis absorption spectra records from 402,034 papers with 18,309 records. We used SMILES to predict category of absorption region (4126 training samples and 1032 test samples).</td><td>C</td></tr><tr><td>Bandgap (Zhuo et al.，2018)</td><td>This dataset is retrieved from Zhuo et al.'s work, containing data on experimental band gaps and DFT calculated zero band gaps for 4604 compounds.</td><td>R</td></tr><tr><td>Steels (Dunn et al.，2020)</td><td>This dataset is retrieved from Citrine informatics, containing data on steel yield strengths from compo- sition for 312 chemical formulas.</td><td>R</td></tr><tr><td>ChEMBL (Gaulton et al.，2012)</td><td>This dataset is sourced from a curated database of bioactive molecules with drug-like properties, focus- ing on the lipophilicity of 1899 molecular compounds in pharmacokinetics. The water-octanol partition co- efficient (logD) is used to describe lipophilicity.</td><td>R</td></tr><tr><td>Diversity (Moosavi et al., 2020)</td><td>A diverse set of structures based on the chemical and geometric descriptors.</td><td>R</td></tr><tr><td>Cp (Moosavi et al.， 2020) ESOL (Delaney，2004)</td><td>Dataset for predicting the heat capacity of materials based on density functional theory simulations. This dataset is a compilation of measured aqueous</td><td>R</td></tr><tr><td></td><td>solubility (LogS) values, a crucial factor in drug dis- covery. The dataset comprises 927 molecular com- pounds originally used for ESOL - estimated solubil- ity.</td><td>T</td></tr></table></body></html>  