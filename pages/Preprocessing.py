import shutil
import streamlit as st
import pandas as pd
import os
import json
import time
from itertools import cycle
import re
from datetime import datetime
from magic_pdf.data.data_reader_writer import FileBasedDataReader
from magic_pdf.data.dataset import PymuDocDataset
from magic_pdf.model.doc_analyze_by_custom_model import doc_analyze
from magic_pdf.config.enums import SupportedPdfParseMethod
from pathlib import Path

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from typing import List, Dict
from langchain_experimental.text_splitter import SemanticChunker
import hashlib
from core.chatbot import chatbot
from typing import List, Dict

st.set_page_config(
    layout="wide",
    initial_sidebar_state="expanded",
)

# é¡µé¢æ ‡é¢˜
st.subheader("æ–‡çŒ®å¤„ç†")
st.divider()

# åˆå§‹åŒ–ç›®å½•å’Œæ–‡ä»¶
os.makedirs("PDF", exist_ok=True)
os.makedirs("Markdown", exist_ok=True)  # æ–°å¢Markdownå­˜å‚¨ç›®å½•
JSONL_PATH = "Jsonfile/metadata.jsonl"
Config_Path = "Jsonfile/em_model_config.json"
# åˆå§‹åŒ–session_stateï¼ˆä»JSONLåŠ è½½å†å²æ•°æ®ï¼‰
if "file_data" not in st.session_state:
    if os.path.exists(JSONL_PATH):
        with open(JSONL_PATH, "r") as f:
            data = [json.loads(line) for line in f]
            # ç¡®ä¿æ¯ä¸ªæ¡ç›®éƒ½æœ‰QA_resultå­—æ®µ
            for entry in data:
                if "QA_result" not in entry:
                    entry["QA_result"] = []
            st.session_state.file_data = pd.DataFrame(data)
    else:
        st.session_state.file_data = pd.DataFrame(columns=[
            "æ ‡é¢˜", 
            "ä¸Šä¼ æ—¶é—´", 
            "å¤§å°", 
            "çŠ¶æ€", 
            "å­˜å‚¨è·¯å¾„",
            "mdè·¯å¾„",
            "å‘é‡åº“è·¯å¾„",
            "å®ä½“æ•°é‡", 
            "å®ä½“",
            "QA_result",
            "Chunksåœ°å€"
            ])

class SmartDocumentProcessor:
    def __init__(self,config_path: str = "Jsonfile/em_model_config.json"):
        with open(config_path, 'r') as f:
            em_config = json.load(f)
        self.embed_model = HuggingFaceEmbeddings(
            # model_name="/home/binbin/deeplearning/MinerU/bge-m3",
            model_name=em_config["model_paths"]["embedding_model"],
            model_kwargs={"device": "cuda"},
            encode_kwargs={"batch_size": 10}
        )

    def _detect_content_type(self, text: str) -> str:
        """å¢å¼ºå‹å†…å®¹ç±»å‹æ£€æµ‹"""
        if re.search(r'```[\s\S]*?```|def |import |print\(', text):
            return "code"
        if re.search(r'\|.*\|.*\n[-:|]+', text):
            return "table"
        if re.search(r'\$.*?\$|\\[({]', text):
            return "formula"
        return "normal"

    def process_documents(self, file_path: str) -> List[Dict]:
        loader = TextLoader(file_path)
        documents = loader.load()

        # è¯­ä¹‰åˆ†å—
        chunker = SemanticChunker(
            embeddings=self.embed_model,
            breakpoint_threshold_amount=82,
            add_start_index=True
        )
        base_chunks = chunker.split_documents(documents)

        # åŠ¨æ€äºŒæ¬¡åˆ†å—
        final_chunks = []
        for chunk in base_chunks:
            content_type = self._detect_content_type(chunk.page_content)
            
            if content_type == "code":
                splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=64)
            elif content_type == "table":
                splitter = RecursiveCharacterTextSplitter(chunk_size=384, chunk_overlap=96)
            else:
                splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=128)
                
            final_chunks.extend(splitter.split_documents([chunk]))

        # æ·»åŠ åµŒå…¥å…ƒæ•°æ®
        for i, chunk in enumerate(final_chunks):
            chunk.metadata.update({
                "chunk_id": f"chunk_{i}_{hashlib.md5(chunk.page_content.encode()).hexdigest()[:8]}",
                "content_type": self._detect_content_type(chunk.page_content),
                "source_file": file_path
            })
        return final_chunks
    
    def save_full_chunks(self, chunks: List[Dict], file_name: str) -> str:
        """ä¿å­˜æ‰€æœ‰chunksåˆ°å•ä¸ªjsonæ–‡ä»¶"""
        chunks_dir = Path("Chunks")
        chunks_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        file_hash = hashlib.md5(file_name.encode()).hexdigest()[:8]
        chunk_name = f"{file_hash}_{Path(file_name).stem}.json"
        chunk_path = chunks_dir / chunk_name
        
        # å†™å…¥å†…å®¹
        with open(chunk_path, "w", encoding="utf-8") as f:
            json.dump([dict(chunk) for chunk in chunks], f, indent=2,ensure_ascii=False)  # å…³é”®ç‚¹ï¼šä¸æ·»åŠ ä»»ä½•é¢å¤–å­—ç¬¦
                
        return str(chunk_path)     

def article_split(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        txt = f.read()
        # print(txt)
    # ä½¿ç”¨#è¿›è¡Œåˆ†å‰²
    slice_ = re.split("#", txt)
    # åˆå¹¶è¾ƒçŸ­éƒ¨åˆ†
    slice_1 = []
    temp = []
    for i in slice_:
        # æ ¹æ®æ¢è¡Œç¬¦æ•°é‡åˆ¤æ–­æ®µè½é•¿åº¦
        if len(re.findall("\n", i)) <= 2:
            temp.append(i)
        elif len(temp) > 0:
            temp.append(i)
            slice_1.append(''.join(temp))
            temp = []
        else:
            slice_1.append(i)
    # æå–æ‘˜è¦ã€ä»‹ç»ã€ç»“è®º
    abstract = None
    introduction = None
    conclusion = None
    # æŸ¥æ‰¾æ‘˜è¦
    for section in slice_1:
        if re.search(r"\babstract\b", section, re.IGNORECASE):
            abstract = section
            break
    # æŸ¥æ‰¾å¼•è¨€
    for section in slice_1:
        if re.search(r"\bintroduction\b", section, re.IGNORECASE) and section != abstract:
            introduction = section
            break
    # æŸ¥æ‰¾ç»“è®ºï¼ˆå€’åºæŸ¥æ‰¾ï¼‰
    for section in reversed(slice_1):
        if re.search(r"\bconclusion\b", section, re.IGNORECASE):
            conclusion = section
            break
    # å¦‚æœæœªæ‰¾åˆ°ç»“è®ºï¼Œå°è¯•åœ¨å‚è€ƒæ–‡çŒ®å‰æŸ¥æ‰¾
    if not conclusion:
        for i in range(len(slice_1)):
            if re.search(r"\breferences\b", slice_1[-1-i], re.IGNORECASE):
                if len(slice_1) > i + 1:
                    conclusion = slice_1[-2-i]
                    break
    return (abstract, introduction, conclusion)

# def chatbot(content):
#     """è°ƒç”¨å¤§æ¨¡å‹APIç”Ÿæˆå›ç­”"""
#     try:
#         client = OpenAI(
#             api_key="sk-c577fd2f34ed4a828b5d6ce320c8fdde",
#             base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
#             http_client=httpx.Client(proxies="http://127.0.0.1:7897")
#         )

#         completion = client.chat.completions.create(
#             model="qwen-plus",
#             messages=[{'role': 'user', 'content': content}],
#             temperature=0.3
#         )
        
#         # elapsed_time = time.time() - start_time
#         answer = completion.choices[0].message.content
#         used_tokens = completion.usage.total_tokens
#         return answer, used_tokens
#     except Exception as e:
#         return f"è¯·æ±‚å¤±è´¥: {str(e)}", 0

def query_generation(entities):
    base_prompt = """\
## Question Generation Requirements:
1. Generate 10 independent questions based on the following academic entities. Questions must focus on the entity's definition, principles, or applications.
2. Strictly avoid mentioning paper content, author research, or contextual information.
3. Prohibit generating questions of the following types:
   - "What is the role of this entity in the paper?"
   - "How did the authors apply this entity?"
   - "Where is this entity mentioned in the paper?"
4. Question types should be diversified, including but not limited to:
   - Concept explanation (What is...)
   - Technical comparison (Differences from...)
   - Application  (How to apply)
   - Historical development (Evolution process)
   - Mathematical principles (How it works/calculates)

## Format Requirements:
["Question 1","Question 2",...,"Question 10"]
## Entity List:
{entities}
Output ONLY the JSON-formatted list of questions that meet the requirements, without any explanations.
"""
    
    formatted_prompt = base_prompt.format(
        entities="\n".join([f"- {e}" for e in entities])
    )
    # logging.info("å¼€å§‹ç”Ÿæˆé—®é¢˜ï¼Œå®ä½“æ•°é‡ï¼š%d", len(entities))
    
    max_retries = 3
    for attempt in range(max_retries):
        try:
            response, _ = chatbot(formatted_prompt)
            
            start_idx = response.find("[")
            end_idx = response.rfind("]") + 1
            questions = json.loads(response[start_idx:end_idx])
            
            if len(questions) != 10:
                raise ValueError("ç”Ÿæˆé—®é¢˜æ•°é‡ä¸è¶³")
                
            cleaned_questions = [
                q.replace("In this paper", "").replace("In this study", "")
                .replace("According to the paper", "").replace("As mentioned in the research", "")
                .replace("the authors'", "").replace("the proposed", "")
                .strip(' ,.;')
                for q in questions
            ]
            
            return cleaned_questions
            
        except (json.JSONDecodeError, ValueError) as e:
            print(f"Attempt {attempt+1} failed, retrying...")
            if attempt == max_retries -1:
                templates = cycle([
                    "What is the scientific definition of {entity}?",
                    "How does {entity} fundamentally operate?",
                    "What are the key components of {entity}?",
                    "What problem space does {entity} primarily address?",
                    "What are the limitations of {entity}?"
                ])
                selected_entities = entities[:10] if len(entities) >= 10 else (entities * (10 // len(entities) + 1))[:10]
                return [
                    next(templates).format(entity=entity)
                    for entity in selected_entities
                ]


colA, colB, colC, colD, colE, colF = st.columns(
    [0.3, 0.14, 0.14, 0.14, 0.14, 0.14], 
    vertical_alignment="center"
)

with colA:
    uploaded_files = st.file_uploader("é€‰æ‹©PDFæ–‡ä»¶", accept_multiple_files=True, type=["pdf"])
    
    for uploaded_file in uploaded_files:
        file_name = uploaded_file.name
        file_size = uploaded_file.size / (1024*1024)
        size_display = f"{file_size:.2f} MB"
        
        # é˜²æ­¢é‡å¤ä¸Šä¼ 
        if not ((st.session_state.file_data["æ ‡é¢˜"] == file_name) & 
                (st.session_state.file_data["å¤§å°"] == size_display)).any():
            
            # ä¿å­˜PDFæ–‡ä»¶
            timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
            saved_name = f"{timestamp}_{file_name}"
            file_path = os.path.join("PDF", saved_name)
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            # æ„å»ºå…ƒæ•°æ®
            metadata = {
                "æ ‡é¢˜": file_name,
                "ä¸Šä¼ æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "å¤§å°": size_display,
                "çŠ¶æ€": "å·²ä¸Šä¼ ",
                "å­˜å‚¨è·¯å¾„": file_path,
                "mdè·¯å¾„": "" , # æ–°å¢markdownè·¯å¾„å­—æ®µ
                "å‘é‡åº“è·¯å¾„":"",
                "å®ä½“æ•°é‡":0,
                "å®ä½“":[],
                "Chunksåœ°å€":"",
                "QA_result":[]
            }
            with open(JSONL_PATH, "a") as f:
                f.write(json.dumps(metadata, ensure_ascii=False) + "\n")
            
            # æ›´æ–°session_state
            st.session_state.file_data = pd.concat(
                [st.session_state.file_data, pd.DataFrame([metadata])],
                ignore_index=True
            )

with colB:
    if st.button("é¢„å¤„ç†", use_container_width=True):
        selection = st.session_state.get("data", {}).get("selection", {})
        selected_rows = selection.get("rows", [])
        
        if not selected_rows:
            st.warning("è¯·å…ˆé€‰æ‹©è¦é¢„å¤„ç†çš„æ–‡ä»¶")
        else:
            progress = st.progress(0)
            status = st.empty()
            errors = []
            total = len(selected_rows)
            
            for idx, row_idx in enumerate(selected_rows, 1):
                file_name = st.session_state.file_data.at[row_idx, "æ ‡é¢˜"]
                status.info(f"æ­£åœ¨å¤„ç† {idx}/{total}: {file_name}")
                progress.progress((idx-1)/total)  # å…ˆæ˜¾ç¤ºå‡†å¤‡çŠ¶æ€
                
                try:
                    # å®é™…å¤„ç†è¿‡ç¨‹å¼€å§‹
                    file_path = st.session_state.file_data.at[row_idx, "å­˜å‚¨è·¯å¾„"]
                    name_without_suff = Path(file_name).stem
                    local_md_dir = "Markdown"
                    os.makedirs(local_md_dir, exist_ok=True)
                    
                    reader = FileBasedDataReader("")
                    pdf_bytes = reader.read(file_path)
                    ds = PymuDocDataset(pdf_bytes)
                    parse_method = ds.classify()
                    infer_result = ds.apply(doc_analyze, ocr=(parse_method == SupportedPdfParseMethod.OCR))
                    
                    if parse_method == SupportedPdfParseMethod.OCR:
                        pipe_result = infer_result.pipe_ocr_mode(None)
                    else:
                        pipe_result = infer_result.pipe_txt_mode(None)
                    
                    md_content = pipe_result.get_markdown("")
                    output_md_path = os.path.join(local_md_dir, f"{name_without_suff}.md")
                    
                    with open(output_md_path, "w", encoding="utf-8") as f:
                        f.write(md_content)
                    
                    # æ›´æ–°å…ƒæ•°æ®
                    st.session_state.file_data.at[row_idx, "çŠ¶æ€"] = "å·²è½¬æ¢"
                    st.session_state.file_data.at[row_idx, "mdè·¯å¾„"] = output_md_path
                    with open(JSONL_PATH, "w") as f:
                        for _, row in st.session_state.file_data.iterrows():
                            json.dump(row.to_dict(), f, ensure_ascii=False)
                            f.write("\n")
                    
                    # å¤„ç†å®Œæˆåæ›´æ–°çŠ¶æ€å’Œè¿›åº¦
                    status.success(f"âœ… {idx}/{total}: {file_name} è½¬æ¢æˆåŠŸ")
                    progress.progress(idx/total)
                    time.sleep(1.5)
                    
                except Exception as e:
                    error_msg = f"âŒ {file_name}: {str(e)}"
                    errors.append(error_msg)
                    status.error(error_msg)
                    st.session_state.file_data.at[row_idx, "çŠ¶æ€"] = "è½¬æ¢å¤±è´¥"
                    progress.progress(idx/total)  # å³ä½¿å¤±è´¥ä¹Ÿè¦æ¨è¿›è¿›åº¦
                
                # å¼ºåˆ¶ç•Œé¢æ›´æ–°ï¼ˆç§»é™¤sleepä¿è¯å®æ—¶æ€§ï¼‰
                status.empty()
                status.write(" ")
            
            # æœ€ç»ˆå¤„ç†
            progress.empty()
            if errors:
                status.error(f"å¤„ç†å®Œæˆï¼ŒæˆåŠŸ {total-len(errors)} ä¸ªï¼Œå¤±è´¥ {len(errors)} ä¸ª")
                with st.expander("æŸ¥çœ‹é”™è¯¯è¯¦æƒ…"):
                    st.error("\n\n".join(errors))
            else:
                status.success("âœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†æˆåŠŸï¼")
            
            st.rerun()

with colC:
    if st.button("æå–å®ä½“", use_container_width=True):
        selection = st.session_state.get("data", {}).get("selection", {})
        selected_rows = selection.get("rows", [])
        if not selected_rows:
            st.warning("è¯·å…ˆé€‰æ‹©è¦æŠ½å–å®ä½“çš„æ–‡ä»¶")
        else:
            progress = st.progress(0)
            status = st.empty()
            errors = []
            total = len(selected_rows)
            
            for idx, row_idx in enumerate(selected_rows, 1):
                try:
                    file_name = st.session_state.file_data.at[row_idx, "æ ‡é¢˜"]
                    md_path = st.session_state.file_data.at[row_idx, "mdè·¯å¾„"]
                    # print(md_path)
                    
                    # æ£€æŸ¥markdownæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    if not os.path.exists(md_path):
                        st.warning(f"Markdownæ–‡ä»¶ {md_path} ä¸å­˜åœ¨ï¼Œè¯·å…ˆé¢„å¤„ç†ï¼")
                        time.sleep(1.5)
                        
                    status.info(f"å¤„ç† {idx}/{total}: {file_name}")
                    progress.progress((idx-1)/total)
                    # æ‰§è¡Œå†…å®¹åˆ‡åˆ†
                    abstract, intro, conclusion = article_split(md_path)
                    # print(abstract,intro,conclusion)
                    
                    # æ„å»ºæç¤ºè¯
                    prompt = """
                    ## Generation Requirements:
                    1. Please extract 15 entities from the above text.
                    2. The format of the generation is as follows:
                    ["Entity 1","Entity 2",...]
                    3. Do not generate any text that is not related to the generation requirements.
                    4. The extracted entities only include academic entities such as concepts and academic terms, and do not include entities that are not related to academics, such as names, dates, and places.
                    Abstractï¼š{abstract}
                    Instructionï¼š{intro}
                    Conclusionï¼š{conclusion}
                    
                    """.format(
                        abstract=abstract or "None",
                        intro=intro or "None",
                        conclusion=conclusion or "None"
                    )
                    # print(prompt)
                    # è°ƒç”¨å¤§æ¨¡å‹API
                    entities_str, token_usage = chatbot(prompt)
                    entities = json.loads(entities_str)
                    
                    # æ›´æ–°å…ƒæ•°æ®
                    st.session_state.file_data.at[row_idx, "å®ä½“"] = entities
                    st.session_state.file_data.at[row_idx, "çŠ¶æ€"] = "å·²æŠ½å–å®ä½“"
                    st.session_state.file_data.at[row_idx, "å®ä½“æ•°é‡"] = len(entities)
                    
                    # é‡å†™JSONLæ–‡ä»¶
                    with open(JSONL_PATH, "w", encoding="utf-8") as f:
                        for _, row in st.session_state.file_data.iterrows():
                            json.dump(row.to_dict(), f, ensure_ascii=False)
                            f.write("\n")
                            
                    status.success(f"âœ… {idx}/{total}: {file_name} æŠ½å–æˆåŠŸï¼ˆ{len(entities)}ä¸ªå®ä½“ï¼‰")
                    progress.progress(idx/total)
                    
                except Exception as e:
                    error_msg = f"âŒ {file_name}: {str(e)}"
                    errors.append(error_msg)
                    st.session_state.file_data.at[row_idx, "çŠ¶æ€"] = "æŠ½å–å¤±è´¥"
                    st.session_state.file_data.at[row_idx, "å®ä½“"] = []
                    progress.progress(idx/total)
                    status.error(error_msg)
                    continue
                finally:
                    time.sleep(0.5)  # é¿å…ç•Œé¢æ›´æ–°è¿‡å¿«
                    
            # æœ€ç»ˆå¤„ç†
            progress.empty()
            if errors:
                status.error(f"å¤„ç†å®Œæˆï¼ŒæˆåŠŸ {total - len(errors)} ä¸ªï¼Œå¤±è´¥ {len(errors)} ä¸ª")
                with st.expander("é”™è¯¯è¯¦æƒ…"):
                    st.error("\n".join(errors))
            else:
                status.success("âœ… æ‰€æœ‰æ–‡ä»¶å®ä½“æŠ½å–å®Œæˆï¼")
            st.rerun()
with colD:
    if st.button("ç”Ÿæˆé—®é¢˜", use_container_width=True):
        selection = st.session_state.get("data", {}).get("selection", {})
        selected_rows = selection.get("rows", [])
        if not selected_rows:
            st.warning("è¯·å…ˆé€‰æ‹©è¦ç”ŸæˆQAçš„æ–‡ä»¶")
        else:
            progress = st.progress(0)
            status = st.empty()
            errors = []
            total = len(selected_rows)
            
            for idx, row_idx in enumerate(selected_rows, 1):
                try:
                    file_name = st.session_state.file_data.at[row_idx, "æ ‡é¢˜"]
                    entities = st.session_state.file_data.at[row_idx, "å®ä½“"]
                    
                    if not entities or len(entities) == 0:
                        raise ValueError("æœªæ‰¾åˆ°å®ä½“ï¼Œè¯·å…ˆæŠ½å–å®ä½“")
                        time.sleep(2)
                    
                    status.info(f"å¤„ç† {idx}/{total}: {file_name}")
                    progress.progress((idx-1)/total)
                    
                    # ç”Ÿæˆé—®é¢˜
                    questions = query_generation(entities)
                    
                    # è½¬æ¢ä¸ºQAç»“æ„
                    qa_list = [{"question": q, "answer": "", "reference": ""} for q in questions]
                    
                    # æ›´æ–°å…ƒæ•°æ®
                    st.session_state.file_data.at[row_idx, "QA_result"] = qa_list
                    st.session_state.file_data.at[row_idx, "çŠ¶æ€"] = "å·²ç”Ÿé—®é¢˜"
                    
                    # æ›´æ–°JSONLæ–‡ä»¶
                    with open(JSONL_PATH, "w", encoding="utf-8") as f:
                        for _, row in st.session_state.file_data.iterrows():
                            json.dump(row.to_dict(), f, ensure_ascii=False)
                            f.write("\n")
                            
                    status.success(f"âœ… {idx}/{total}: {file_name} ç”ŸæˆæˆåŠŸï¼ˆ{len(questions)}ä¸ªé—®é¢˜ï¼‰")
                    progress.progress(idx/total)
                    
                except Exception as e:
                    error_msg = f"âŒ {file_name}: {str(e)}"
                    errors.append(error_msg)
                    st.session_state.file_data.at[row_idx, "çŠ¶æ€"] = "Qç”Ÿæˆå¤±è´¥"
                    progress.progress(idx/total)
                    status.error(error_msg)
                finally:
                    time.sleep(0.5)
                    
            progress.empty()
            if errors:
                status.error(f"å¤„ç†å®Œæˆï¼ŒæˆåŠŸ {total - len(errors)} ä¸ªï¼Œå¤±è´¥ {len(errors)} ä¸ª")
                with st.expander("é”™è¯¯è¯¦æƒ…"):
                    st.error("\n".join(errors))
            else:
                status.success("âœ… æ‰€æœ‰æ–‡ä»¶QAç”Ÿæˆå®Œæˆ!è¯·è¿›å…¥QAç®¡ç†é¡µé¢æŸ¥çœ‹")
                time.sleep(2)
            st.rerun()
with colE:
    if st.button("æ–‡æœ¬åµŒå…¥", use_container_width=True):
        selection = st.session_state.get("data", {}).get("selection", {})
        selected_rows = selection.get("rows", [])
        
        if not selected_rows:
            st.warning("è¯·å…ˆé€‰æ‹©è¦åµŒå…¥çš„æ–‡ä»¶")
        else:
            progress = st.progress(0)
            status = st.empty()
            errors = []
            total = len(selected_rows)
            
            for idx, row_idx in enumerate(selected_rows, 1):
                try:
                    file_name = st.session_state.file_data.at[row_idx, "æ ‡é¢˜"]
                    md_path = st.session_state.file_data.at[row_idx, "mdè·¯å¾„"]
                    
                    # æ£€æŸ¥é¢„å¤„ç†çŠ¶æ€
                    if st.session_state.file_data.at[row_idx, "mdè·¯å¾„"] == "":
                        raise ValueError("è¯·å…ˆå®Œæˆé¢„å¤„ç†")
                    
                    status.info(f"å¤„ç†ä¸­ {idx}/{total}: {file_name}")
                    progress.progress((idx-1)/total)
                    
                    # ç”Ÿæˆå”¯ä¸€å‘é‡åº“è·¯å¾„
                    file_hash = hashlib.md5(file_name.encode()).hexdigest()[:8]
                    vector_db_path = f"vector_db/{file_hash}_{Path(file_name).stem}"
                    
                    # å¤„ç†æ–‡æ¡£
                    processor = SmartDocumentProcessor()
                    chunks = processor.process_documents(md_path)

                    chunks_json_path = processor.save_full_chunks(chunks, file_name)
                    st.session_state.file_data.at[row_idx, "Chunksåœ°å€"] = chunks_json_path
                    
                    # åˆ›å»ºå‘é‡åº“
                    with open(Config_Path, 'r') as f:
                        em_config = json.load(f)
                    Chroma.from_documents(
                        documents=chunks,
                        embedding=HuggingFaceEmbeddings(model_name=em_config["model_paths"]["embedding_model"]),
                        persist_directory=vector_db_path
                    )
                    
                    # æ›´æ–°å…ƒæ•°æ®
                    st.session_state.file_data.at[row_idx, "çŠ¶æ€"] = "å·²åµŒå…¥"
                    st.session_state.file_data.at[row_idx, "å‘é‡åº“è·¯å¾„"] = vector_db_path
                    
                    # æ›´æ–°JSONL
                    with open(JSONL_PATH, "w", encoding="utf-8") as f:
                        for _, row in st.session_state.file_data.iterrows():
                            json.dump(row.to_dict(), f, ensure_ascii=False)
                            f.write("\n")
                            
                    status.success(f"âœ… {idx}/{total}: {file_name} åµŒå…¥æˆåŠŸ")
                    progress.progress(idx/total)
                    
                except Exception as e:
                    error_msg = f"âŒ {file_name}: {str(e)}"
                    errors.append(error_msg)
                    st.session_state.file_data.at[row_idx, "çŠ¶æ€"] = "åµŒå…¥å¤±è´¥"
                    progress.progress(idx/total)
                    status.error(error_msg)
                finally:
                    time.sleep(0.5)
                    
            progress.empty()
            if errors:
                status.error(f"å¤„ç†å®Œæˆï¼ŒæˆåŠŸ {total-len(errors)} ä¸ªï¼Œå¤±è´¥ {len(errors)} ä¸ª")
                with st.expander("é”™è¯¯è¯¦æƒ…"):
                    st.error("\n".join(errors))
            else:
                    status.success("âœ… æ‰€æœ‰æ–‡ä»¶åµŒå…¥å®Œæˆï¼")
            st.rerun()
with colF:
    if st.button("åˆ é™¤", use_container_width=True):
        selection = st.session_state.get("data", {}).get("selection", {})
        selected_rows = selection.get("rows", [])
        if not selected_rows:
            st.warning("è¯·å…ˆé€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶")
        else:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€ä¸ªæ•°æ®
            is_last_record = len(st.session_state.file_data) == len(selected_rows)
            
            # é€†åºåˆ é™¤é¿å…ç´¢å¼•é”™ä½
            for row_idx in sorted(selected_rows, reverse=True):
                # åˆ é™¤PDFæ–‡ä»¶
                pdf_path = st.session_state.file_data.at[row_idx, "å­˜å‚¨è·¯å¾„"]
                try:
                    os.remove(pdf_path)
                except Exception as e:
                    st.error(f"åˆ é™¤PDFæ–‡ä»¶ {pdf_path} å¤±è´¥: {e}")
                
                # åˆ é™¤Markdownæ–‡ä»¶
                md_path = st.session_state.file_data.at[row_idx, "mdè·¯å¾„"]
                if md_path:  # ç¡®ä¿è·¯å¾„å­˜åœ¨
                    try:
                        os.remove(md_path)
                    except FileNotFoundError:
                        st.warning(f"Markdownæ–‡ä»¶ {md_path} ä¸å­˜åœ¨")
                    except Exception as e:
                        st.error(f"åˆ é™¤Markdownæ–‡ä»¶ {md_path} å¤±è´¥: {e}")
                # åˆ é™¤å‘é‡åº“
                vector_db_path = st.session_state.file_data.at[row_idx, "å‘é‡åº“è·¯å¾„"]
                if vector_db_path:  # ç¡®ä¿è·¯å¾„å­˜åœ¨
                    try:
                        shutil.rmtree(vector_db_path)
                    except FileNotFoundError:
                        st.warning(f"å‘é‡åº“ {vector_db_path} ä¸å­˜åœ¨")
                    except Exception as e:
                        st.error(f"åˆ é™¤å‘é‡åº“ {vector_db_path} å¤±è´¥: {e}")

                chunks_txt_path = st.session_state.file_data.at[row_idx, "Chunksåœ°å€"]
                if chunks_txt_path: 
                    try:
                        os.remove(chunks_txt_path)
                    except FileNotFoundError:
                        st.warning(f"Chunksåœ°å€ {chunks_txt_path} ä¸å­˜åœ¨")
                    except Exception as e:
                        st.error(f"åˆ é™¤Chunksåœ°å€ {chunks_txt_path} å¤±è´¥: {e}")

                # ä»session_stateä¸­ç§»é™¤è®°å½•
                st.session_state.file_data = st.session_state.file_data.drop(row_idx)
            
            # é‡ç½®ç´¢å¼•å¹¶æ›´æ–°JSONL
            st.session_state.file_data = st.session_state.file_data.reset_index(drop=True)
            
            # å¦‚æœæ˜¯æœ€åä¸€ä¸ªè®°å½•ï¼Œå®Œå…¨åˆ é™¤JSONLæ–‡ä»¶
            if is_last_record:
                try:
                    os.remove(JSONL_PATH)
                    st.success("æ‰€æœ‰æ–‡ä»¶å·²åˆ é™¤ï¼Œå…ƒæ•°æ®æ–‡ä»¶å·²æ¸…é™¤")
                except Exception as e:
                    st.error(f"åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
            else:
                # é‡å†™JSONLæ–‡ä»¶
                with open(JSONL_PATH, "w") as f:
                    for _, row in st.session_state.file_data.iterrows():
                        json.dump(row.to_dict(), f, ensure_ascii=False)
                        f.write("\n")
                st.success("é€‰ä¸­çš„æ–‡ä»¶åŠå…³è”æ–‡ä»¶å·²åˆ é™¤")
            
            st.rerun()  # é‡æ–°åŠ è½½é¡µé¢æ›´æ–°æ˜¾ç¤º


display_columns = ["æ ‡é¢˜", "ä¸Šä¼ æ—¶é—´", "å¤§å°", "çŠ¶æ€", "å­˜å‚¨è·¯å¾„", "mdè·¯å¾„","å‘é‡åº“è·¯å¾„","å®ä½“æ•°é‡","å®ä½“"]
# åœ¨æ˜¾ç¤ºè¡¨æ ¼å‰æ·»åŠ 
st.session_state.file_data["å®ä½“"] = st.session_state.file_data["å®ä½“"].apply(lambda x: x if isinstance(x, list) else [])
pdftable = st.dataframe(
    st.session_state.file_data[display_columns],
    key="data",
    height=600,
    on_select="rerun",
    selection_mode=["multi-row", "multi-column"],
    hide_index=True
)

selection = st.session_state.get("data", {}).get("selection", {})
selected_rows = selection.get("rows", [])

# ä¿®æ”¹åçš„é¢„è§ˆéƒ¨åˆ†ä»£ç 
if selected_rows:
    # å¼ºåˆ¶å•é€‰å¤„ç†ï¼ˆåªå–ç¬¬ä¸€ä¸ªé€‰ä¸­çš„è¡Œï¼‰
    if len(selected_rows) > 1:
        st.warning("âš ï¸ è¯·é€‰æ‹©å•ä¸ªæ–‡ä»¶è¿›è¡Œé¢„è§ˆ")
        selected_rows = [selected_rows[0]]  # å¼ºåˆ¶å–ç¬¬ä¸€ä¸ªé€‰ä¸­é¡¹
        st.session_state.data["selection"]["rows"] = selected_rows  # æ›´æ–°é€‰ä¸­çŠ¶æ€

    if st.button("é¢„è§ˆé€‰ä¸­æ–‡ä»¶"):
        st.session_state.show_preview = True

# åŠ¨æ€å“åº”é€‰æ‹©çŠ¶æ€å˜åŒ–
if not selected_rows and st.session_state.get("show_preview"):
    st.session_state.show_preview = False
    st.rerun()

if st.session_state.get("show_preview"):
    # è·å–é€‰ä¸­æ–‡ä»¶ä¿¡æ¯
    selected_index = selected_rows[0]
    selected_file = st.session_state.file_data.iloc[selected_index].to_dict()
    
    # é¢„è§ˆé¢æ¿
    with st.container(border=True):
        col_close = st.columns([0.95, 0.05])
        with col_close[1]:
            if st.button("âœ•", type="secondary"):
                st.session_state.show_preview = False
                st.rerun()
        
        # åŒæ å¸ƒå±€
        col_pdf, col_md = st.columns([0.5, 0.5], gap="small")
        
        with col_pdf:
            st.subheader("PDF")
            pdf_path = selected_file["å­˜å‚¨è·¯å¾„"]
            
            if os.path.exists(pdf_path):
                try:
                    import fitz
                    
                    doc = fitz.open(pdf_path)
                    page_count = len(doc)
                    
                    # æ›´ç´§å‡‘çš„é¡µé¢æ§åˆ¶
                    cols = st.columns([0.1, 0.3, 0.1, 0.5])
                    with cols[1]:
                        selected_page = st.number_input(
                            "é¡µç ",
                            min_value=1,
                            max_value=page_count,
                            value=1,
                            label_visibility="collapsed"
                        )
                    # æ·»åŠ å·¦å³ç®­å¤´æŒ‰é’®
                    with cols[0]:
                        if st.button("â†"):
                            selected_page = max(1, selected_page-1)
                    
                    with cols[2]:
                        if st.button("â†’"):
                            selected_page = min(page_count, selected_page+1)
                    
                    # æ˜¾ç¤ºå½“å‰é¡µ/æ€»é¡µæ•°
                    cols[3].caption(f"Page {selected_page}/{page_count}")
                    
                    # é¡µé¢æ¸²æŸ“
                    with st.spinner("æ­£åœ¨æ¸²æŸ“..."):
                        page = doc.load_page(selected_page-1)
                        pix = page.get_pixmap(dpi=150)  # æé«˜DPI
                        img_data = pix.tobytes()
                        st.image(img_data, use_container_width =True)

                except Exception as e:
                    st.error(f"PDFé¢„è§ˆå¤±è´¥: {str(e)}")

        with col_md:
            st.subheader("Markdown")
            md_path = selected_file["mdè·¯å¾„"]

            # if md_path and os.path.exists(md_path):
            #     # åˆå§‹åŒ–sessionçŠ¶æ€
            #     if 'edited_md' not in st.session_state:
            #         with open(md_path, "r", encoding="utf-8") as f:
            #             st.session_state.edited_md = f.read()
            if md_path and os.path.exists(md_path):
                # è·å–å½“å‰é€‰ä¸­æ–‡ä»¶çš„MDè·¯å¾„
                current_md_path = selected_file["mdè·¯å¾„"]
                
                # å½“åˆ‡æ¢æ–‡ä»¶æˆ–é¦–æ¬¡åŠ è½½æ—¶æ›´æ–°å†…å®¹
                if 'preview_md_path' not in st.session_state or st.session_state.preview_md_path != current_md_path:
                    with open(current_md_path, "r", encoding="utf-8") as f:
                        st.session_state.edited_md = f.read()
                    st.session_state.preview_md_path = current_md_path  # è®°å½•å½“å‰è·¯å¾„
                
                # åˆ›å»ºç¼–è¾‘ç•Œé¢
                with st.container(height=1025):

                    edited = st.text_area(
                        "ç¼–è¾‘å†…å®¹",
                        value=st.session_state.edited_md,
                        height=900,
                        label_visibility="collapsed",
                        key="md_editor"
                    )
                    
                    if st.button("ğŸ’¾ ä¿å­˜ä¿®æ”¹"):
                        try:
                            with open(md_path, "w", encoding="utf-8") as f:
                                f.write(edited)
                            st.session_state.edited_md = edited
                            st.success("ä¿å­˜æˆåŠŸï¼")
                            time.sleep(1)
                            st.rerun()
                        except Exception as e:
                            st.error(f"ä¿å­˜å¤±è´¥: {str(e)}")
            else:
                st.warning("Markdownæ–‡ä»¶æœªç”Ÿæˆ")
