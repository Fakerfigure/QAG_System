import streamlit as st
import pandas as pd
import os
import json
import time
from pathlib import Path
from typing import List, Dict
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from sentence_transformers import CrossEncoder
from langchain.docstore.document import Document  
import torch
import re
from core.chatbot import chatbot

st.set_page_config(
    layout="wide",
    initial_sidebar_state="expanded",
)

# é¡µé¢æ ‡é¢˜
st.subheader("QAç®¡ç†")
st.divider()

# åˆå§‹åŒ–ç›®å½•å’Œæ–‡ä»¶
JSONL_PATH = "Jsonfile/metadata.jsonl"

# åˆå§‹åŒ–session_stateï¼ˆä»JSONLåŠ è½½å†å²æ•°æ®ï¼‰
if "file_data" not in st.session_state:
    if os.path.exists(JSONL_PATH):
        with open(JSONL_PATH, "r") as f:
            data = [json.loads(line) for line in f]
            # æ•°æ®å…¼å®¹æ€§å¤„ç†
            for entry in data:
                entry.setdefault("QA_result", [])
            st.session_state.file_data = pd.DataFrame(data)
    else:
        st.session_state.file_data = pd.DataFrame(columns=[
            "æ ‡é¢˜", 
            "ä¸Šä¼ æ—¶é—´", 
            "å¤§å°", 
            "çŠ¶æ€", 
            "å­˜å‚¨è·¯å¾„",
            "mdè·¯å¾„",
            "å‘é‡åº“è·¯å¾„",
            "å®ä½“æ•°é‡", 
            "å®ä½“",
            "QA_result"
            "Chunksåœ°å€"
            ])

def save_to_jsonl():
    """å°†å½“å‰æ•°æ®ä¿å­˜å›JSONLæ–‡ä»¶"""
    with open(JSONL_PATH, "w") as f:
        for _, row in st.session_state.file_data.iterrows():
            json_line = json.dumps(row.to_dict(), ensure_ascii=False)
            f.write(json_line + "\n")

# æ£€ç´¢å™¨ç±»
class HybridRetriever:
    def __init__(self, chunks: List[Dict], persist_dir: str,config_path: str = "Jsonfile/em_model_config.json"):
        """åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨"""
        # self.logger = logging.getLogger(self.__class__.__name__)
        with open(config_path, 'r') as f:
            em_config = json.load(f)
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.vector_db = Chroma.from_documents(
            documents=chunks,
            # embedding=HuggingFaceEmbeddings(model_name="/home/binbin/deeplearning/MinerU/bge-m3"),
            embedding=HuggingFaceEmbeddings(model_name=em_config["model_paths"]["embedding_model"]),
            persist_directory=persist_dir
        )
        
        # BM25æ£€ç´¢å™¨
        self.bm25_retriever = BM25Retriever.from_documents(chunks, k=5)
        
        # æ··åˆæ£€ç´¢å™¨
        self.ensemble_retriever = EnsembleRetriever(
            retrievers=[
                self.vector_db.as_retriever(search_kwargs={"k": 5}),
                self.bm25_retriever
            ],
            weights=[0.6, 0.4]  
        )
        
        # é‡æ’åºæ¨¡å‹
        self.reranker = CrossEncoder(
            # "/home/binbin/deeplearning/MinerU/bge-reranker-large", 
            em_config["model_paths"]["reranker_model"],
            device="cuda" if torch.cuda.is_available() else "cpu"
        )

    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:
        """ä¸¤é˜¶æ®µæ£€ç´¢æµç¨‹"""
        # self.logger.info(f"å¤„ç†æŸ¥è¯¢: {query}")
        
        # ç¬¬ä¸€é˜¶æ®µï¼šæ··åˆæ£€ç´¢
        docs = self.ensemble_retriever.get_relevant_documents(query)
        
        # ç¬¬äºŒé˜¶æ®µï¼šé‡æ’åº
        pairs = [[query, doc.page_content] for doc in docs]
        scores = self.reranker.predict(pairs)
        ranked_docs = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)

        return [doc for doc, _ in ranked_docs[:top_k]]

# def chatbot(content):
#     """è°ƒç”¨å¤§æ¨¡å‹APIç”Ÿæˆå›ç­”"""
#     try:
#         client = OpenAI(
#             api_key="sk-c577fd2f34ed4a828b5d6ce320c8fdde",
#             base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
#             http_client=httpx.Client(proxies="http://127.0.0.1:7897")
#         )

#         completion = client.chat.completions.create(
#             model="qwen-plus",
#             messages=[{'role': 'user', 'content': content}],
#             temperature=0.3
#         )
        
#         # elapsed_time = time.time() - start_time
#         answer = completion.choices[0].message.content
#         used_tokens = completion.usage.total_tokens
#         return answer, used_tokens
#     except Exception as e:
#         return f"è¯·æ±‚å¤±è´¥: {str(e)}", 0

# RAGç³»ç»Ÿ
class EnhancedRAG:
    def __init__(self, chunks: List[Dict], persist_dir: str):
        self.retriever = HybridRetriever(chunks, persist_dir)

    def generate_prompt(self, question: str, contexts: List[Dict]) -> str:
        """ç”Ÿæˆç»“æ„åŒ–æç¤º"""
        context_entries = []
        for doc in contexts:
            meta = getattr(doc, 'metadata', {})
            context_entries.append(
                f"[æ¥æºï¼š{meta.get('source_file', 'unknown')}ï¼Œ"
                f"ç±»å‹ï¼š{meta.get('content_type', 'normal')}]\n"
                f"{doc.page_content}"
            )
        context_str = "\n\n".join(context_entries)

        return f"""# Role Definition
You are the Chief Expert of a world-leading economic management think tank, possessing the following core competencies:
1. Profound expertise in cutting-edge theories, empirical research methods, and policy analysis in economic management
2. Exceptional ability to translate complex economic models into actionable business insights
3. Precision in identifying statistical significance and practical relevance in research data
4. Cross-disciplinary integration capabilities (finance, econometrics, strategic management, etc.)

# Knowledge Repository
The following knowledge units have undergone rigorous academic validation:
{context_str}

# Problem Statement
"{question}"

# Generate evidence-based response using knowledge repository
"""

    def ask(self, question: str) -> dict:
        """é—®ç­”æµç¨‹"""
        contexts = self.retriever.retrieve(question)
        prompt = self.generate_prompt(question, contexts)
        
        answer, used_tokens = chatbot(prompt)
        
        # æ„å»ºå¼•ç”¨æ¥æº
        references = []
        for doc in contexts:
            meta = getattr(doc, 'metadata', {})
            references.append(
                f"æ¥æºæ–‡ä»¶: {meta.get('source_file', 'unknown')}\n"
                f"å†…å®¹ç±»å‹: {meta.get('content_type', 'normal')}\n"
                f"åŸæ–‡ç‰‡æ®µ: {doc.page_content}"
            )
        reference_str = "\n\n".join(references)
        
        return {
            "answer": answer.strip(),
            "reference": reference_str,
            # "tokens": used_tokens
        }

# é¡µé¢æ“ä½œæŒ‰é’®
colA, colB, colC = st.columns([0.3, 0.3, 0.3], vertical_alignment="center")
with colA:
    generate_btn = st.button("ç”Ÿæˆç­”æ¡ˆ", help="ä¸ºé€‰ä¸­é—®é¢˜ç”Ÿæˆç­”æ¡ˆ")
    if generate_btn:
        processed_files = total_processed = 0
        
        with st.spinner("å¤„ç†ä¸­..."):
            try:
                for idx, row in st.session_state.file_data.iterrows():
                    df_key = f"qa_df_{idx}"
                    selected_rows = st.session_state.get(df_key, {}).get("selection", {}).get("rows", [])
                    if not selected_rows:
                        continue

                    # åŠ è½½å¹¶è½¬æ¢chunksæ•°æ®
                    chunks_path = row.get("Chunksåœ°å€", "")
                    if not os.path.exists(chunks_path):
                        st.warning(f"è·³è¿‡ {row['æ ‡é¢˜']}: chunksæ–‡ä»¶ä¸å­˜åœ¨")
                        continue
                    
                    with open(chunks_path, "r") as f:
                        raw_chunks = json.load(f)
                        chunks = [
                            Document(
                                page_content=item["page_content"],
                                metadata=item.get("metadata", {})
                            ) for item in raw_chunks
                        ]

                    # åˆå§‹åŒ–RAG
                    rag = EnhancedRAG(chunks, row["å‘é‡åº“è·¯å¾„"])
                    
                    # å¤„ç†é€‰ä¸­é—®é¢˜
                    updated_qa = row["QA_result"].copy()
                    for qa_idx in selected_rows:
                        if qa_idx < len(updated_qa):
                            result = rag.ask(updated_qa[qa_idx]["question"])
                            updated_qa[qa_idx].update(result)
                            total_processed += 1
                    
                    st.session_state.file_data.at[idx, "QA_result"] = updated_qa
                    processed_files += 1
                    
                save_to_jsonl()
                st.success(f"æˆåŠŸå¤„ç†{processed_files}ä¸ªæ–‡ä»¶ï¼Œç”Ÿæˆ{total_processed}ä¸ªç­”æ¡ˆ")
                st.rerun()
                
            except Exception as e:
                st.error(f"å¤„ç†å¤±è´¥: {str(e)}")
with colB:
    delete_btn = st.button("åˆ é™¤QA", type="secondary", help="åˆ é™¤é€‰ä¸­æ–‡ç« çš„æ‰€æœ‰QAè®°å½•")
    # åˆ é™¤é€»è¾‘å¤„ç†
    if delete_btn:
        try:
            updated_data = []
            deleted_count = 0
            
            for idx, row in st.session_state.file_data.iterrows():
                df_key = f"qa_df_{idx}"
                
                # è·å–å½“å‰æ–‡ç« çš„é€‰ä¸­è¡Œ
                if df_key in st.session_state:
                    selected_rows = st.session_state[df_key].get("selection", {}).get("rows", [])
                    
                    # è¿‡æ»¤ä¿ç•™æœªé€‰ä¸­çš„QAé¡¹
                    if selected_rows:
                        new_qa = [qa for qa_idx, qa in enumerate(row["QA_result"]) if qa_idx not in selected_rows]
                        deleted_count += len(selected_rows)
                    else:
                        new_qa = row["QA_result"]
                    
                    # æ›´æ–°è¡Œæ•°æ®
                    updated_row = row.copy()
                    updated_row["QA_result"] = new_qa
                    updated_data.append(updated_row)
                else:
                    updated_data.append(row)
            
            if deleted_count > 0:
                # æ›´æ–°session_state
                st.session_state.file_data = pd.DataFrame(updated_data)
                
                # æŒä¹…åŒ–ä¿å­˜
                save_to_jsonl()

                st.success(f"æˆåŠŸåˆ é™¤ {deleted_count} ä¸ªQAé¡¹")
                time.sleep(1.5)
                st.rerun()
            else:
                st.warning("æœªé€‰æ‹©è¦åˆ é™¤çš„QAé¡¹")
                time.sleep(1.5)
                
        except Exception as e:
            st.error(f"åˆ é™¤æ“ä½œå¤±è´¥: {str(e)}")
            time.sleep(1.5)
            import traceback
            traceback.print_exc()
with colC: 
    DBcreate = st.button("åˆ›å»ºæ•°æ®é›†", type="secondary", help="å°†é€‰ä¸­çš„QAä¿å­˜ä¸ºæ–°çš„æ•°æ®é›†æ–‡ä»¶")
    if DBcreate:
        st.session_state.show_dataset_dialog = True  # è§¦å‘æ˜¾ç¤ºå¯¹è¯æ¡†

    # å¤„ç†æ•°æ®é›†åˆ›å»ºå¯¹è¯æ¡†
    if st.session_state.get("show_dataset_dialog"):
        with st.form(key="dataset_creation_form"):
            dataset_name = st.text_input("æ•°æ®é›†åç§°ï¼ˆæ— éœ€åç¼€ï¼‰", key="dataset_name", 
                                       help="è¯·è¾“å…¥è‹±æ–‡åç§°ï¼Œä¸è¦åŒ…å«ç‰¹æ®Šå­—ç¬¦")
                    # ä½¿ç”¨åˆ—æ¥æ¨ªå‘æ’åˆ—æŒ‰é’®
            col1, col2 = st.columns([1, 3])  # è°ƒæ•´æ¯”ä¾‹ä¸º1:3ä½¿æŒ‰é’®é å·¦
            with col1:
                submit = st.form_submit_button("åˆ›å»º")
            with col2:
                cancel = st.form_submit_button("å–æ¶ˆ")
            # submit = st.form_submit_button("åˆ›å»º")
            # cancel = st.form_submit_button("å–æ¶ˆ")
            if submit:
                if not dataset_name.strip():
                    st.error("æ•°æ®é›†åç§°ä¸èƒ½ä¸ºç©º!")
                else:
                    # æ”¶é›†æ‰€æœ‰é€‰ä¸­çš„QAé¡¹
                    selected_entries = []
                    for idx, row in st.session_state.file_data.iterrows():
                        df_key = f"qa_df_{idx}"
                        if df_key in st.session_state:
                            selected_rows = st.session_state[df_key].get("selection", {}).get("rows", [])
                            for qa_idx in selected_rows:
                                if qa_idx < len(row["QA_result"]):
                                    qa = row["QA_result"][qa_idx]
                                    selected_entries.append({
                                        "æ ‡é¢˜": row["æ ‡é¢˜"],
                                        "question": qa["question"],
                                        "answer": qa["answer"]
                                    })
                    
                    if not selected_entries:
                        st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªQAé¡¹!")
                    else:
                        # åˆ›å»ºä¿å­˜ç›®å½•
                        save_dir = Path("DataBase_manage")
                        save_dir.mkdir(parents=True, exist_ok=True)
                        
                        # ç”Ÿæˆåˆæ³•æ–‡ä»¶å
                        valid_name = re.sub(r'[\\/:*?"<>|]', "", dataset_name.strip())
                        file_path = save_dir / f"{valid_name}.jsonl"
                        
                        # å†™å…¥JSONLæ–‡ä»¶
                        try:
                            with open(file_path, "w", encoding="utf-8") as f:
                                for entry in selected_entries:
                                    json_line = json.dumps(entry, ensure_ascii=False)
                                    f.write(json_line + "\n")
                            st.success(f"æ•°æ®é›†å·²åˆ›å»º: {file_path}")
                            time.sleep(1.5)
                            del st.session_state.show_dataset_dialog
                            st.rerun()
                        except Exception as e:
                            st.error(f"ä¿å­˜å¤±è´¥: {str(e)}")

            if cancel:
                del st.session_state.show_dataset_dialog
                st.rerun()


# å±•ç¤ºæ‰€æœ‰æ–‡ç« çš„QAæ•°æ®
for idx, row in st.session_state.file_data.iterrows():
    with st.expander(f"ğŸ“„ {row['æ ‡é¢˜']} - QAæ•°é‡: {len(row['QA_result'])}", expanded=True):
        if not row.get("QA_result"):
            st.info("è¯¥æ–‡ç« å°šæœªç”ŸæˆQAå†…å®¹")
            continue
            
        # è½¬æ¢ä¸ºäº¤äº’å¼DataFrame
        qa_df = pd.DataFrame(row["QA_result"])[["question", "answer", "reference"]]
        qa_df.columns = ["é—®é¢˜", "ç­”æ¡ˆ", "å‚è€ƒæ–‡çŒ®"]
        
        # åˆ›å»ºå”¯ä¸€é”®å€¼
        df_key = f"qa_df_{idx}"
        
        # æ˜¾ç¤ºå¯äº¤äº’è¡¨æ ¼
        selection = st.dataframe(
            qa_df,
            use_container_width=True,
            key=df_key,
            on_select="rerun",
            selection_mode="multi-row",
            hide_index=True,
            column_config={
                "é—®é¢˜": {"width": "40%"},
                "ç­”æ¡ˆ": {"width": "40%"},
                "å‚è€ƒæ–‡çŒ®": {"width": "20%"}
            }
        )
        # æ·»åŠ ç¼–è¾‘åŠŸèƒ½
        df_key = f"qa_df_{idx}"
        selected_rows = st.session_state.get(df_key, {}).get("selection", {}).get("rows", [])

        # åˆ¤æ–­å½“å‰æ˜¯å¦å¤„äºç¼–è¾‘æ¨¡å¼
        is_editing = st.session_state.get("editing", {}).get("article_idx") == idx

        if not is_editing:
            # æ˜¾ç¤ºç¼–è¾‘æŒ‰é’®ï¼ˆä»…å½“é€‰ä¸­ä¸€ä¸ªQAæ—¶ï¼‰
            if len(selected_rows) == 1:
                edit_col, _ = st.columns([0.2, 0.8])
                with edit_col:
                    if st.button("åŒå‡»ç¼–è¾‘é€‰ä¸­QA", key=f"edit_btn_{idx}"):
                        st.session_state.editing = {
                            "article_idx": idx,
                            "qa_idx": selected_rows[0],
                            "original_question": row["QA_result"][selected_rows[0]]["question"],
                            "original_answer": row["QA_result"][selected_rows[0]]["answer"]
                        }

        # æ˜¾ç¤ºç¼–è¾‘è¡¨å•
        if is_editing and st.session_state.editing["article_idx"] == idx:
            qa_idx = st.session_state.editing["qa_idx"]
            current_qa = row["QA_result"][qa_idx]

            with st.form(key=f"edit_form_{idx}"):
                # åˆ›å»ºä¸¤åˆ—å¸ƒå±€

                new_question = st.text_area(
                        "é—®é¢˜ç¼–è¾‘",
                        value=st.session_state.editing["original_question"],
                        height=90,
                        key=f"question_edit_{idx}"
                    )
                col1, col2 = st.columns([0.6, 0.4])

                with col1:
                    new_answer = st.text_area(
                        "ç­”æ¡ˆç¼–è¾‘", 
                        value=st.session_state.editing["original_answer"],
                        height=300,
                        key=f"answer_edit_{idx}"
                    )

                with col2:
                    st.text_area(
                        "å‚è€ƒæ–‡çŒ®",
                        value=current_qa["reference"],
                        height=300,
                        disabled=True,
                        key=f"reference_edit_{idx}"
                    )
                    # st.markdown(f"```\n{current_qa['reference']}\n```", help="æ­¤å†…å®¹ä¸å¯ç¼–è¾‘")

                # æŒ‰é’®å¸ƒå±€
                btn_col1, btn_col2,_,_,_ = st.columns([0.2, 0.2,0.2,0.2,0.2])
                with btn_col1:
                    save_btn = st.form_submit_button("ğŸ’¾ ä¿å­˜ä¿®æ”¹")
                with btn_col2:
                    cancel_btn = st.form_submit_button("âœ•")

                if save_btn:
                    # æ‰§è¡Œæ•°æ®æ›´æ–°
                    updated_qa = row["QA_result"].copy()
                    updated_qa[qa_idx]["question"] = new_question
                    updated_qa[qa_idx]["answer"] = new_answer
                    
                    # æ›´æ–°sessionçŠ¶æ€
                    st.session_state.file_data.at[idx, "QA_result"] = updated_qa
                    save_to_jsonl()  # ä¿å­˜åˆ°æ–‡ä»¶
                    del st.session_state.editing  # é€€å‡ºç¼–è¾‘æ¨¡å¼
                    st.success("ä¿®æ”¹å·²ä¿å­˜ï¼")
                    time.sleep(1)
                    st.rerun()

                if cancel_btn:
                    del st.session_state.editing
                    st.rerun()




